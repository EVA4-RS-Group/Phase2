# Session 5 - Monocular Human Pose Estimation


## 1. Executive Summary
**Group Members:** *Ramjee Ganti, Srinivasan G, Roshan, Dr. Rajesh and Sujit Ojha*

### **Objectives**:

- You are implementing "[Simple Baseline for HPE and tracking](https://github.com/Microsoft/human-pose-estimation.pytorch)". Read the [paper](https://arxiv.org/pdf/1804.06208.pdf) and write a detailed readme file describing the model architecture as well as the JointsMSELoss class.
- Download the [smallest model](https://onedrive.live.com/?authkey=%21AFkTgCsr3CT9%2D%5FA&id=56B9F9C97F261712%2110709&cid=56B9F9C97F261712) and upload to Lambda for HPE detection
- Make sure to draw the points on the image, as well as connect the joints in the right fashion.

### **Results**:

- Team hosted static website : http://rsgroup.s3-website.ap-south-1.amazonaws.com/
- Website results
    - <img src="results/img7.png" alt="Elon Musk1" height="700"/><img src="results/img6.png" alt="Elon Musk2" height="700"/>
- Colab results
    - <img src="results/mk1.png" alt="Elon Musk Orginal" height="300"/><img src="results/mk2.png" alt="Elon Musk with HPE" height="300"/>


### **Simple Baseline for HPC and Tracking - Model Architecture and JointsMSELoss**

![Image](https://github.com/EVA4-RS-Group/Phase2/blob/master/S5_HumanPoseEstimation/results/simple_pose.png)


- The model proposed simply adds 3 deconvolutional layers over the last convolution stage in the ResNet backbone network, called C5.   
- This structure is simplest to generate heatmaps from deep and low resolution features.
- This model also adopted in the state-of-the-art Mask R-CNN
- It is a bottom-up approach, since the model is going to predict the probability of joints as heatmaps.

- 3 deconvolutional layers with batch normalization (BN) and ReLU activation are used. 
- Each layer has 256 filters with kernel size (4 × 4) with a stride of 2.
- A (1 × 1) convolutional layer is added at last to generate predicted heatmaps for all 16 key points.

Mean Squared Error (MSE) is used as the loss between the predicted heatmaps and targeted heatmaps. 
The targeted heatmap for 16 key points are generated by applying a 2D gaussian centered on the key point ground truth location.

This method combines the upsampling and convolutional parameters into deconvolutional layers in a much simpler way,
without using skip layer connections.

3 upsampling steps and 3 levels of non-linearity (from the deepest feature) are used to obtain highresolution feature maps and heatmaps.

Obtaining high resolution feature maps is crucial whatever the way it is obtained.
In the following code we can appriciate its network and understand its implementation

Code: [pose_resnet.py](src/pose_resnet.py)

### JointsMSELoss

The loss is a simple 0.5 \* sqrt(joint_coord - target_coord)^2 , but also the target coordinates of the joint in the ground truth can have weights assigned to them, so that specific joints can be trained more/ penalised more.

When using the [MPII Dataset](http://human-pose.mpi-inf.mpg.de/), there are going to be 16 joints predicted, i.e. 16 layers in the output of the model.

Code: [loss.py](src/loss.py)


Here, the max values in the heatmaps are obtained first, this will most probably give the right prediction, but we can do further post-processing to get the exact centre of the prediction, which is done by applying 2D gaussian centered on the heatmap.


### **Key Highlights**
- Model Conversion & Inferencing
    - Defined an inference class to manage the joints, pairs, defining the model, image transformation and model conversion. **Bascially an modularized code.**
    - ONNX Conversion, Model converted to ONNX format with int8 quantization which resulted in **~2x reduction in model size (136 mb to 66 mb) and inference speed improvement by ~1.6x(519.18 ms to 316.61 ms)**
- Plotting using openCV
    - Heatmaps of each joints are converted into point using openCV min_max point with probability. Used openCV elllipse and line function to draw the joints and join the joints with given color pattern
- Deployment
    - ONNX quantilized model deployed on AWS Lambda and model is inferred using ONNX runtime.
    - Image being resized and normalized using opencv and numpy instead of torchvision.transforms().


## 2. Steps (Developer Section)
- Model Conversion & Inferencing, HPE [EVA4_P2_S5_HumanPoseEstimation_ONNX_Quant_v1.ipynb](EVA4_P2_S5_HumanPoseEstimation_ONNX_Quant_v1.ipynb)
    - Loading HPE model (pose_resnet_50_256x256.pth.tar) with configuration file (256x256_d256x3_adam_lr1e-3.yaml)
    - Defining inferencing class [inference.py](src/inference.py) and [inference_onnx.py](src/inference_onnx.py)
        - Joint Definition based on MPII data set
        ```python 
        JOINTS = ['r-ankle', 'r-knee', 'r-hip', 'l-hip',
        'l-knee', 'l-ankle', 'pelvis', 'thorax',
        'upper-neck', 'head-top', 'r-wrist', 'r-elbow',
        'r-shoulder', 'l-shoulder', 'l-elbow', 'l-wrist'] 
        ```
        - Define the pose pair and color set
        ```python
        POSE_PAIRS = [
            # UPPER BODY
            [9, 8],[8, 7],[7, 3],[7, 2],
            # LOWER BODY
            [6, 2],[2, 1],[1, 0],[6, 3],[3, 4],[4, 5],
            # ARMS
            [8, 12],[12, 11],[11, 10],[8, 13],[13, 14],[14, 15]]

        POSE_PAIRS_COL = [
            # UPPER BODY
            (65,190,115),(50,55,235),(110,230,255),(195,125,40), 
            # LOWER BODY
            (180,75,160),(255,225,110),(65,190,115),(180,75,160),(50,55,240),(195,125,40), 
            # ARMS
            (180,75,160),(110,230,255), (195,125,40),
            (255,225,110),(50,55,240),(65,190,115)]
        ```
        - Image transformation : Resizing to 256x256, Normalization using imagenet mean and std.
        - Loading and infering the model to generate output with 16 channel representing each joint heatmap with 64x64 pixel size.
    - Plotting Heatmap
        - Resizing the output channel to 256x256
        - Overlay and plot with input image with transparency set as allpha = 0.5 
        - Create grid of 16 image for each heatmap for indiviual joints.
    - Plotting Human pose connecting joints using openCV lines & ellipses
        - Keypoints are generated using cv2.minMaxLoc to find the max location in each channel with probabilities
        - Keypoints x,y co-ordinate are scaled from 64x64 size to 256x256.
        - Each pose pair are plotting using color defined. (cv2.line)
        - Each Keypoints are plotted using white color. (cv2.ellipse)
    - Model Conversion
        - Model is converted to ONNX using pytorch export function
        - Using ONNX runtime quantization to quantize the model (8bit)
- Deployment [handler.py](HPE-Deployment/handler.py)
    - ONNX quantilized model deployed on AWS Lambda and model is inferred using ONNX runtime.
    - Image being resized and normalized using opencv and numpy instead of torchvision.transforms().

## 3. References

1. [Simple Baseline for HPE and tracking](https://github.com/Microsoft/human-pose-estimation.pytorch)
2. [Human Pose Estimation Blog by Satyajitghana](https://medium.com/@satyajitghana7/human-pose-estimation-and-quantization-of-pytorch-to-onnx-models-a-detailed-guide-b9c91ddc0d9f)
3. [EVA4 Phase2 Session5, Human Pose Estimation](https://theschoolof.ai/)
4. [PyTorch YoloV4 ONNX](https://github.com/Tianxiaomo/pytorch-YOLOv4)
5. [AlexNet to ONNX](https://michhar.github.io/convert-pytorch-onnx/)
6. [ResNet to ONNX](https://colab.research.google.com/github/bentoml/gallery/blob/master/onnx/resnet50/resnet50.ipynb)
7. [SuperResolution to ONNX](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)
