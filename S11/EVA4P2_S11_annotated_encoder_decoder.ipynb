{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "EVA4P2_S11_annotated_encoder_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uOC5RQCeU89n",
        "6kaKjV4lU892",
        "n4D2viY8U892"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EVA4-RS-Group/Phase2/blob/master/S11/EVA4P2_S11_annotated_encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKbVIAtU88S"
      },
      "source": [
        "# The Annotated Encoder-Decoder with Attention\n",
        "\n",
        "Recently, Alexander Rush wrote a blog post called [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html), describing the Transformer model from the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762). This post can be seen as a **prequel** to that: *we will implement an Encoder-Decoder with Attention* using (Gated) Recurrent Neural Networks, very closely following the original attention-based neural machine translation paper [\"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/abs/1409.0473) of Bahdanau et al. (2015). \n",
        "\n",
        "The idea is that going through both blog posts will make you familiar with two very influential sequence-to-sequence architectures. If you have any comments or suggestions, please let me know: [@bastings_nlp](https://twitter.com/bastings_nlp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bed6Z_KEU88U"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "We will model the probability $p(Y\\mid X)$ of a target sequence $Y=(y_1, \\dots, y_{N})$ given a source sequence $X=(x_1, \\dots, x_M)$ directly with a neural network: an Encoder-Decoder.\n",
        "\n",
        "<img src=\"https://github.com/bastings/annotated_encoder_decoder/blob/master/images/bahdanau.png?raw=1\" width=\"636\">\n",
        "\n",
        "#### Encoder \n",
        "\n",
        "The encoder reads in the source sentence (*at the bottom of the figure*) and produces a sequence of hidden states $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$, one for each source word. These states should capture the meaning of a word in its context of the given sentence.\n",
        "\n",
        "We will use a bi-directional recurrent neural network (Bi-RNN) as the encoder; a Bi-GRU in particular.\n",
        "\n",
        "First of all we **embed** the source words. \n",
        "We simply look up the **word embedding** for each word in a (randomly initialized) lookup table.\n",
        "We will denote the word embedding for word $i$ in a given sentence with $\\mathbf{x}_i$.\n",
        "By embedding words, our model may exploit the fact that certain words (e.g. *cat* and *dog*) are semantically similar, and can be processed in a similar way.\n",
        "\n",
        "Now, how do we get hidden states $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$? A forward GRU reads the source sentence left-to-right, while a backward GRU reads it right-to-left.\n",
        "Each of them follows a simple recursive formula: \n",
        "$$\\mathbf{h}_j = \\text{GRU}( \\mathbf{x}_j , \\mathbf{h}_{j - 1} )$$\n",
        "i.e. we obtain the next state from the previous state and the current input word embedding.\n",
        "\n",
        "The hidden state of the forward GRU at time step $j$ will know what words **precede** the word at that time step, but it doesn't know what words will follow. In contrast, the backward GRU will only know what words **follow** the word at time step $j$. By **concatenating** those two hidden states (*shown in blue in the figure*), we get $\\mathbf{h}_j$, which captures word $j$ in its full sentence context.\n",
        "\n",
        "\n",
        "#### Decoder \n",
        "\n",
        "The decoder (*at the top of the figure*) is a GRU with hidden state $\\mathbf{s_i}$. It follows a similar formula to the encoder, but takes one extra input $\\mathbf{c}_{i}$ (*shown in yellow*).\n",
        "\n",
        "$$\\mathbf{s}_{i} = f( \\mathbf{s}_{i - 1}, \\mathbf{y}_{i - 1}, \\mathbf{c}_i )$$\n",
        "\n",
        "Here, $\\mathbf{y}_{i - 1}$ is the previously generated target word (*not shown*).\n",
        "\n",
        "At each time step, an **attention mechanism** dynamically selects that part of the source sentence that is most relevant for predicting the current target word. It does so by comparing the last decoder state with each source hidden state. The result is a context vector $\\mathbf{c_i}$ (*shown in yellow*).\n",
        "Later the attention mechanism is explained in more detail.\n",
        "\n",
        "After computing the decoder state $\\mathbf{s}_i$, a non-linear function $g$ (which applies a [softmax](https://en.wikipedia.org/wiki/Softmax_function)) gives us the probability of the target word $y_i$ for this time step:\n",
        "\n",
        "$$ p(y_i \\mid y_{<i}, x_1^M) = g(\\mathbf{s}_i, \\mathbf{c}_i, \\mathbf{y}_{i - 1})$$\n",
        "\n",
        "Because $g$ applies a softmax, it provides a vector the size of the output vocabulary that sums to 1.0: it is a distribution over all target words. During test time, we would select the word with the highest probability for our translation.\n",
        "\n",
        "Now, for optimization, a [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) is used to maximize the probability of selecting the correct word at this time step. All parameters (including word embeddings) are then updated to maximize this probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSw39SXOU88Y"
      },
      "source": [
        "# Prelims\n",
        "\n",
        "This tutorial requires **PyTorch >= 0.4.1** and was tested with **Python 3.6**.  \n",
        "\n",
        "Make sure you have those versions, and install the packages below if you don't have them yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGlDkt72U88Z"
      },
      "source": [
        "#!pip install torch numpy matplotlib sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR9KsXvoW_uk",
        "outputId": "82df72cd-b813-45aa-885d-ed540fa72533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch==1.5.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (703.8MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8MB 26kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu101) (1.18.5)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed torch-1.5.0+cu101\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4DNbkW5U88g",
        "outputId": "3bf5d685-4f5b-4542-dc67-f35a86d9842f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOgRXyuBU88n"
      },
      "source": [
        "# Let's start coding!\n",
        "\n",
        "## Model class\n",
        "\n",
        "Our base model class `EncoderDecoder` is very similar to the one in *The Annotated Transformer*.\n",
        "\n",
        "One difference is that our encoder also returns its final states (`encoder_final` below), which is used to initialize the decoder RNN. We also provide the sequence lengths as the RNNs require those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaqZdp4iU88o"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
        "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
        "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
        "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask, src_lengths):\n",
        "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
        "    \n",
        "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
        "               decoder_hidden=None):\n",
        "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
        "                            src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZOhVRAxU88v"
      },
      "source": [
        "To keep things easy we also keep the `Generator` class the same. \n",
        "It simply projects the pre-output layer ($x$ in the `forward` function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KytxtJVcU88w"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9pH-egcU882"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "Our encoder is a bi-directional GRU. \n",
        "\n",
        "Because we want to process multiple sentences at the same time for speed reasons (it is more effcient on GPU), we need to support **mini-batches**. Sentences in a mini-batch may have different lengths, which means that the RNN needs to unroll further for certain sentences while it might already have finished for others:\n",
        "\n",
        "```\n",
        "Example: mini-batch with 3 source sentences of different lengths (7, 5, and 3).\n",
        "End-of-sequence is marked with a \"3\" here, and padding positions with \"1\".\n",
        "\n",
        "+---------------+\n",
        "| 4 5 9 8 7 8 3 |\n",
        "+---------------+\n",
        "| 5 4 8 7 3 1 1 |\n",
        "+---------------+\n",
        "| 5 8 3 1 1 1 1 |\n",
        "+---------------+\n",
        "```\n",
        "You can see that, when computing hidden states for this mini-batch, for sentence #2 and #3 we will need to stop updating the hidden state after we have encountered \"3\". We don't want to incorporate the padding values (1s).\n",
        "\n",
        "Luckily, PyTorch has convenient helper functions called `pack_padded_sequence` and `pad_packed_sequence`.\n",
        "These functions take care of masking and padding, so that the resulting word representations are simply zeros after a sentence stops.\n",
        "\n",
        "The code below reads in a source sentence (a sequence of word embeddings) and produces the hidden states.\n",
        "It also returns a final vector, a summary of the complete sentence, by concatenating the first and the last hidden states (they have both seen the whole sentence, each in a different direction). We will use the final vector to initialize the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Jl3dx6U883"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiD1zDu9U889"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is a conditional GRU. Rather than starting with an empty state like the encoder, its initial hidden state results from a projection of the encoder final vector. \n",
        "\n",
        "#### Training\n",
        "In `forward` you can find a for-loop that computes the decoder hidden states one time step at a time. \n",
        "Note that, during training, we know exactly what the target words should be! (They are in `trg_embed`.) This means that we are not even checking here what the prediction is! We simply feed the correct previous target word embedding to the GRU at each time step. This is called teacher forcing.\n",
        "\n",
        "The `forward` function returns all decoder hidden states and pre-output vectors. Elsewhere these are used to compute the loss, after which the parameters are updated.\n",
        "\n",
        "#### Prediction\n",
        "For prediction time, for forward function is only used for a single time step. After predicting a word from the returned pre-output vector, we can call it again, supplying it the word embedding of the previously predicted word and the last state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHwvTvVyU88-"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        \n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))            \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN8vxpyrU89G"
      },
      "source": [
        "### Attention                                                                                                                                                                               \n",
        "\n",
        "At every time step, the decoder has access to *all* source word representations $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$. \n",
        "An attention mechanism allows the model to focus on the currently most relevant part of the source sentence.\n",
        "The state of the decoder is represented by GRU hidden state $\\mathbf{s}_i$.\n",
        "So if we want to know which source word representation(s) $\\mathbf{h}_j$ are most relevant, we will need to define a function that takes those two things as input.\n",
        "\n",
        "Here we use the MLP-based, additive attention that was used in Bahdanau et al.:\n",
        "\n",
        "<img src=\"https://github.com/bastings/annotated_encoder_decoder/blob/master/images/attention.png?raw=1\" width=\"280\">\n",
        "\n",
        "\n",
        "We apply an MLP with tanh-activation to both the current decoder state $\\bf s_i$ (the *query*) and each encoder state $\\bf h_j$ (the *key*), and then project this to a single value (i.e. a scalar) to get the *attention energy* $e_{ij}$. \n",
        "\n",
        "Once all energies are computed, they are normalized by a softmax so that they sum to one: \n",
        "\n",
        "$$ \\alpha_{ij} = \\text{softmax}(\\mathbf{e}_i)[j] $$\n",
        "\n",
        "$$\\sum_j \\alpha_{ij} = 1.0$$ \n",
        "\n",
        "The context vector for time step $i$ is then a weighted sum of the encoder hidden states (the *values*):\n",
        "$$\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WFg-b0IU89J"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOC5RQCeU89n"
      },
      "source": [
        "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
        "We use learned embeddings to convert the input tokens and output tokens to vectors of dimension `emb_size`.\n",
        "\n",
        "We will simply use PyTorch's [nn.Embedding](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N34PK4RU89n"
      },
      "source": [
        "## Full Model\n",
        "\n",
        "Here we define a function from hyperparameters to a full model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH4zftK-U89o"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "\n",
        "    attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
        "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
        "        nn.Embedding(src_vocab, emb_size),\n",
        "        nn.Embedding(tgt_vocab, emb_size),\n",
        "        Generator(hidden_size, tgt_vocab))\n",
        "\n",
        "    return model.cuda() if USE_CUDA else model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbIjsQSU89u"
      },
      "source": [
        "# Training\n",
        "\n",
        "This section describes the training regime for our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVLFzKrPU89v"
      },
      "source": [
        "We stop for a quick interlude to introduce some of the tools \n",
        "needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as their lengths and masks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYiE8QS-U89v"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4tigFmU89w"
      },
      "source": [
        "class Batch:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        \n",
        "        src, src_lengths = src\n",
        "        \n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()\n",
        "                "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22erzpxLU89y"
      },
      "source": [
        "## Training Loop\n",
        "The code below trains the model for 1 epoch (=1 pass through the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NTVCYCGU89z"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
        "    \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    print_tokens = 0\n",
        "\n",
        "    for i, batch in enumerate(data_iter, 1):\n",
        "        \n",
        "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
        "                                           batch.src_mask, batch.trg_mask,\n",
        "                                           batch.src_lengths, batch.trg_lengths)\n",
        "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        print_tokens += batch.ntokens\n",
        "        \n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
        "            start = time.time()\n",
        "            print_tokens = 0\n",
        "\n",
        "    return math.exp(total_loss / float(total_tokens))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kaKjV4lU892"
      },
      "source": [
        "## Training Data and Batching\n",
        "\n",
        "We will use torch text for batching. This is discussed in more detail below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4D2viY8U892"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "We will use the [Adam optimizer](https://arxiv.org/abs/1412.6980) with default settings ($\\beta_1=0.9$, $\\beta_2=0.999$ and $\\epsilon=10^{-8}$).\n",
        "\n",
        "We will use $0.0003$ as the learning rate here, but for different problems another learning rate may be more appropriate. You will have to tune that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZBBYUXU893"
      },
      "source": [
        "# A First  Example\n",
        "\n",
        "We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJmL9ih2U894"
      },
      "source": [
        "## Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeKNYEjcU895"
      },
      "source": [
        "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
        "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
        "    for i in range(num_batches):\n",
        "        data = torch.from_numpy(\n",
        "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
        "        data[:, 0] = sos_index\n",
        "        data = data.cuda() if USE_CUDA else data\n",
        "        src = data[:, 1:]\n",
        "        trg = data\n",
        "        src_lengths = [length-1] * batch_size\n",
        "        trg_lengths = [length] * batch_size\n",
        "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W47-myqU899"
      },
      "source": [
        "## Loss Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Ba5XPOU899"
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1))\n",
        "        loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            loss.backward()          \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss.data.item() * norm"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttV8PgxU8-A"
      },
      "source": [
        "### Printing examples\n",
        "\n",
        "To monitor progress during training, we will translate a few examples.\n",
        "\n",
        "We use greedy decoding for simplicity; that is, at each time step, starting at the first token, we choose the one with that maximum probability, and we never revisit that choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJ6bEfhU8-A"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    attention_scores = []\n",
        "    hidden = None\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out, hidden, pre_output = model.decode(\n",
        "              encoder_hidden, encoder_final, src_mask,\n",
        "              prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            prob = model.generator(pre_output[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
        "    \n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    # (only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output, np.concatenate(attention_scores, axis=1)\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab=None):\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "\n",
        "    return [str(t) for t in x]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3azWlP-U8-D"
      },
      "source": [
        "def print_examples(example_iter, model, n=2, max_len=100, \n",
        "                   sos_index=1, \n",
        "                   src_eos_index=None, \n",
        "                   trg_eos_index=None, \n",
        "                   src_vocab=None, trg_vocab=None):\n",
        "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print()\n",
        "    \n",
        "    if src_vocab is not None and trg_vocab is not None:\n",
        "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
        "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
        "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
        "    else:\n",
        "        src_eos_index = None\n",
        "        trg_sos_index = 1\n",
        "        trg_eos_index = None\n",
        "        \n",
        "    for i, batch in enumerate(example_iter):\n",
        "      \n",
        "        src = batch.src.cpu().numpy()[0, :]\n",
        "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
        "\n",
        "        # remove </s> (if it is there)\n",
        "        src = src[:-1] if src[-1] == src_eos_index else src\n",
        "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
        "      \n",
        "        result, _ = greedy_decode(\n",
        "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
        "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
        "        print(\"Example #%d\" % (i+1))\n",
        "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
        "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
        "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
        "        print()\n",
        "        \n",
        "        count += 1\n",
        "        if count == n:\n",
        "            break"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFdEVnHHU8-H"
      },
      "source": [
        "## Training the copy task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oyg77ga6U8-H"
      },
      "source": [
        "def train_copy_task():\n",
        "    \"\"\"Train the simple copy task.\"\"\"\n",
        "    num_words = 11\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
        "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
        " \n",
        "    dev_perplexities = []\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    for epoch in range(20):\n",
        "        \n",
        "        print(\"Epoch %d\" % epoch)\n",
        "\n",
        "        # train\n",
        "        model.train()\n",
        "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
        "        run_epoch(data, model,\n",
        "                  SimpleLossCompute(model.generator, criterion, optim))\n",
        "\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad(): \n",
        "            perplexity = run_epoch(eval_data, model,\n",
        "                                   SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
        "            dev_perplexities.append(perplexity)\n",
        "            print_examples(eval_data, model, n=2, max_len=9)\n",
        "        \n",
        "    return dev_perplexities"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "H3oge_gZU8-K",
        "outputId": "43b47277-9c84-4916-f2a3-71a270eed68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the copy task\n",
        "dev_perplexities = train_copy_task()\n",
        "\n",
        "def plot_perplexity(perplexities):\n",
        "    \"\"\"plot perplexities\"\"\"\n",
        "    plt.title(\"Perplexity per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.plot(perplexities)\n",
        "    \n",
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch Step: 50 Loss: 19.720032 Tokens per Sec: 13131.025630\n",
            "Epoch Step: 100 Loss: 17.850552 Tokens per Sec: 11889.159715\n",
            "Evaluation perplexity: 7.165516\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  5 8 7 5 8 7 5 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 8 8 8 8 8 8 8\n",
            "\n",
            "Epoch 1\n",
            "Epoch Step: 50 Loss: 15.429652 Tokens per Sec: 15189.534161\n",
            "Epoch Step: 100 Loss: 11.758206 Tokens per Sec: 14298.635003\n",
            "Evaluation perplexity: 3.761093\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 5 3 8 7 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 8 2 5 8 3 2\n",
            "\n",
            "Epoch 2\n",
            "Epoch Step: 50 Loss: 9.917424 Tokens per Sec: 12941.066808\n",
            "Epoch Step: 100 Loss: 8.949948 Tokens per Sec: 15331.891377\n",
            "Evaluation perplexity: 2.573576\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 7 8 10\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 8 5 2 6 8 2\n",
            "\n",
            "Epoch 3\n",
            "Epoch Step: 50 Loss: 7.275529 Tokens per Sec: 14714.561472\n",
            "Epoch Step: 100 Loss: 6.582402 Tokens per Sec: 13311.908143\n",
            "Evaluation perplexity: 2.072120\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 8 7 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 2 5 8 2 6\n",
            "\n",
            "Epoch 4\n",
            "Epoch Step: 50 Loss: 5.942050 Tokens per Sec: 12386.528748\n",
            "Epoch Step: 100 Loss: 4.906625 Tokens per Sec: 14832.949997\n",
            "Evaluation perplexity: 1.765160\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 10 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 5\n",
            "Epoch Step: 50 Loss: 5.266299 Tokens per Sec: 15670.337367\n",
            "Epoch Step: 100 Loss: 4.140486 Tokens per Sec: 14722.585178\n",
            "Evaluation perplexity: 1.565405\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 10 5 7 8\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 6\n",
            "Epoch Step: 50 Loss: 3.958579 Tokens per Sec: 13000.358942\n",
            "Epoch Step: 100 Loss: 3.681249 Tokens per Sec: 12827.919926\n",
            "Evaluation perplexity: 1.455613\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 5 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 8 6\n",
            "\n",
            "Epoch 7\n",
            "Epoch Step: 50 Loss: 2.987804 Tokens per Sec: 13345.327797\n",
            "Epoch Step: 100 Loss: 2.790762 Tokens per Sec: 15493.740371\n",
            "Evaluation perplexity: 1.366846\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 8\n",
            "Epoch Step: 50 Loss: 2.148365 Tokens per Sec: 13857.587777\n",
            "Epoch Step: 100 Loss: 2.303622 Tokens per Sec: 15705.817484\n",
            "Evaluation perplexity: 1.275108\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 9\n",
            "Epoch Step: 50 Loss: 2.088929 Tokens per Sec: 15660.736027\n",
            "Epoch Step: 100 Loss: 2.050286 Tokens per Sec: 15587.077673\n",
            "Evaluation perplexity: 1.195517\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 10\n",
            "Epoch Step: 50 Loss: 1.499797 Tokens per Sec: 15159.419586\n",
            "Epoch Step: 100 Loss: 1.330637 Tokens per Sec: 15665.228461\n",
            "Evaluation perplexity: 1.150942\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 11\n",
            "Epoch Step: 50 Loss: 1.367972 Tokens per Sec: 15728.320518\n",
            "Epoch Step: 100 Loss: 0.930529 Tokens per Sec: 15604.168313\n",
            "Evaluation perplexity: 1.120272\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 12\n",
            "Epoch Step: 50 Loss: 1.467537 Tokens per Sec: 15552.796198\n",
            "Epoch Step: 100 Loss: 1.062083 Tokens per Sec: 14904.932859\n",
            "Evaluation perplexity: 1.097558\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 13\n",
            "Epoch Step: 50 Loss: 0.660124 Tokens per Sec: 15939.695684\n",
            "Epoch Step: 100 Loss: 0.994441 Tokens per Sec: 15631.690950\n",
            "Evaluation perplexity: 1.065222\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 14\n",
            "Epoch Step: 50 Loss: 0.619340 Tokens per Sec: 15635.413845\n",
            "Epoch Step: 100 Loss: 0.642609 Tokens per Sec: 14184.065454\n",
            "Evaluation perplexity: 1.058820\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 15\n",
            "Epoch Step: 50 Loss: 0.800413 Tokens per Sec: 15626.671896\n",
            "Epoch Step: 100 Loss: 0.400342 Tokens per Sec: 15498.555071\n",
            "Evaluation perplexity: 1.036365\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 16\n",
            "Epoch Step: 50 Loss: 0.258018 Tokens per Sec: 15359.921875\n",
            "Epoch Step: 100 Loss: 0.307210 Tokens per Sec: 15765.401303\n",
            "Evaluation perplexity: 1.026806\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 17\n",
            "Epoch Step: 50 Loss: 0.346129 Tokens per Sec: 13792.701453\n",
            "Epoch Step: 100 Loss: 0.261947 Tokens per Sec: 15158.445595\n",
            "Evaluation perplexity: 1.020942\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 18\n",
            "Epoch Step: 50 Loss: 0.309515 Tokens per Sec: 12864.428579\n",
            "Epoch Step: 100 Loss: 0.164800 Tokens per Sec: 15365.611424\n",
            "Evaluation perplexity: 1.017094\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 19\n",
            "Epoch Step: 50 Loss: 0.244545 Tokens per Sec: 15572.039621\n",
            "Epoch Step: 100 Loss: 0.326245 Tokens per Sec: 15667.646342\n",
            "Evaluation perplexity: 1.017613\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G2E5Vc4U8-O"
      },
      "source": [
        "You can see that the model managed to correctly 'translate' the two examples in the end.\n",
        "\n",
        "Moreover, the perplexity of the development data nicely went down towards 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YkpATcuU8-P"
      },
      "source": [
        "# A Real World Example\n",
        "\n",
        "Now we consider a real-world example using the IWSLT German-English Translation task. \n",
        "This task is much smaller than usual, but it illustrates the whole system. \n",
        "\n",
        "The cell below installs torch text and spacy. This might take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxldzW2U8-P",
        "outputId": "1ef5dc8a-d003-4d73-911c-fca2adb79533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+git://github.com/pytorch/text spacy \n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/text\n",
            "  Cloning git://github.com/pytorch/text to /tmp/pip-req-build-u6oo9ujb\n",
            "  Running command git clone -q git://github.com/pytorch/text /tmp/pip-req-build-u6oo9ujb\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+94ec092) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+94ec092) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+94ec092) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+94ec092) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+94ec092) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+94ec092) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+94ec092) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+94ec092) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+94ec092) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtext: filename=torchtext-0.9.0a0+94ec092-cp36-cp36m-linux_x86_64.whl size=7089234 sha256=0e0f36c3d65a4f8d1736c1b59defc4da7b57b400f8aaf370964d9fb828f63b3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-malf3htl/wheels/39/42/ff/82f5ccbb0f30b25e14610376f5d0c67913fc05017dab59f8eb\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.9.0a0+94ec092\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=16500146d80a622ab171e28f4236879ed19c9c98b531282aa28b48c43ebf839c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vmsut58l/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyoCideeU8-S"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "We will load the dataset using torchtext and spacy for tokenization.\n",
        "\n",
        "This cell might take a while to run the first time, as it will download and tokenize the IWSLT data.\n",
        "\n",
        "For speed we only include short sentences, and we include a word in the vocabulary only if it occurs at least 5 times. In this case we also lowercase the data.\n",
        "\n",
        "If you have **issues** with torch text in the cell below (e.g. an `ascii` error), try running `export LC_ALL=\"en_US.UTF-8\"` before you start `jupyter notebook`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDiwbILUU8-S",
        "outputId": "a26193d6-f06a-4693-a97d-6e4bfed57dcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    UNK_TOKEN = \"<unk>\"\n",
        "    PAD_TOKEN = \"<pad>\"    \n",
        "    SOS_TOKEN = \"<s>\"\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    LOWER = True\n",
        "    \n",
        "    # we include lengths to provide to the RNNs\n",
        "    SRC = data.Field(tokenize=tokenize_de, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
        "    TRG = data.Field(tokenize=tokenize_en, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
        "\n",
        "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
        "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
        "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
        "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
        "    \n",
        "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:04<00:00, 5.17MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7QoqqJ5U8-V"
      },
      "source": [
        "### Let's look at the data\n",
        "\n",
        "It never hurts to look at your data and some statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OHv8ug0U8-W",
        "outputId": "812b2f9a-813c-44e1-d71e-0f89abd5dee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
        "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
        "\n",
        "    print(\"Data set sizes (number of sentence pairs):\")\n",
        "    print('train', len(train_data))\n",
        "    print('valid', len(valid_data))\n",
        "    print('test', len(test_data), \"\\n\")\n",
        "\n",
        "    print(\"First training example:\")\n",
        "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
        "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
        "\n",
        "    print(\"Most common words (src):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "    print(\"Most common words (trg):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "\n",
        "    print(\"First 10 words (src):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
        "    print(\"First 10 words (trg):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
        "\n",
        "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
        "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
        "    \n",
        "    \n",
        "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set sizes (number of sentence pairs):\n",
            "train 143115\n",
            "valid 690\n",
            "test 963 \n",
            "\n",
            "First training example:\n",
            "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
            "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
            "\n",
            "Most common words (src):\n",
            "         .     138329\n",
            "         ,     105944\n",
            "       und      41843\n",
            "       die      40808\n",
            "       das      33324\n",
            "       sie      33034\n",
            "       ich      31150\n",
            "       ist      31037\n",
            "        es      27449\n",
            "       wir      25817 \n",
            "\n",
            "Most common words (trg):\n",
            "         .     137259\n",
            "         ,      91615\n",
            "       the      73343\n",
            "       and      50276\n",
            "        to      42799\n",
            "         a      39572\n",
            "        of      39496\n",
            "         i      33521\n",
            "        it      32920\n",
            "      that      32640 \n",
            "\n",
            "First 10 words (src):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 </s>\n",
            "03 .\n",
            "04 ,\n",
            "05 und\n",
            "06 die\n",
            "07 das\n",
            "08 sie\n",
            "09 ich \n",
            "\n",
            "First 10 words (trg):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 <s>\n",
            "03 </s>\n",
            "04 .\n",
            "05 ,\n",
            "06 the\n",
            "07 and\n",
            "08 to\n",
            "09 a \n",
            "\n",
            "Number of German words (types): 15765\n",
            "Number of English words (types): 13002 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baXLuKSaU8-Z"
      },
      "source": [
        "## Iterators\n",
        "Batching matters a ton for speed. We will use torch text's BucketIterator here to get batches containing sentences of (almost) the same length.\n",
        "\n",
        "#### Note on sorting batches for RNNs in PyTorch\n",
        "\n",
        "For effiency reasons, PyTorch RNNs require that batches have been sorted by length, with the longest sentence in the batch first. For training, we simply sort each batch. \n",
        "For validation, we would run into trouble if we want to compare our translations with some external file that was not sorted. Therefore we simply set the validation batch size to 1, so that we can keep it in the original order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB-HdgsAU8-Z",
        "outputId": "2d211e63-12a4-41f5-c775-dc0ee92d59b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_iter = data.BucketIterator(train_data, batch_size=64, train=True, \n",
        "                                 sort_within_batch=True, \n",
        "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
        "                                 device=DEVICE)\n",
        "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
        "                           device=DEVICE)\n",
        "\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
        "    return Batch(batch.src, batch.trg, pad_idx)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrU9k-rOU8-d"
      },
      "source": [
        "## Training the System\n",
        "\n",
        "Now we train the model. \n",
        "\n",
        "On a Titan X GPU, this runs at ~18,000 tokens per second with a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l7kpATSU8-e"
      },
      "source": [
        "def train(model, num_epochs=15, lr=0.0003, print_every=100):\n",
        "    \"\"\"Train a model on IWSLT\"\"\"\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    # optionally add label smoothing; see the Annotated Transformer\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    dev_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        print(\"Epoch\", epoch)\n",
        "        model.train()\n",
        "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
        "                                     model,\n",
        "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
        "                                     print_every=print_every)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
        "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
        "\n",
        "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
        "                                       model, \n",
        "                                       SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
        "            dev_perplexities.append(dev_perplexity)\n",
        "        \n",
        "    return dev_perplexities\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "M8vCBUPaU8-h",
        "outputId": "be1994e7-87cc-4a0c-912a-454a582051ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "dev_perplexities = train(model, print_every=100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 100 Loss: 93.817955 Tokens per Sec: 20646.754931\n",
            "Epoch Step: 200 Loss: 28.690170 Tokens per Sec: 23378.063735\n",
            "Epoch Step: 300 Loss: 79.187950 Tokens per Sec: 24033.277080\n",
            "Epoch Step: 400 Loss: 82.967056 Tokens per Sec: 24478.495257\n",
            "Epoch Step: 500 Loss: 107.953529 Tokens per Sec: 24174.742133\n",
            "Epoch Step: 600 Loss: 43.657337 Tokens per Sec: 23386.104453\n",
            "Epoch Step: 700 Loss: 105.126511 Tokens per Sec: 22847.947305\n",
            "Epoch Step: 800 Loss: 101.920380 Tokens per Sec: 24416.471562\n",
            "Epoch Step: 900 Loss: 54.036655 Tokens per Sec: 23753.330791\n",
            "Epoch Step: 1000 Loss: 52.269363 Tokens per Sec: 24464.706493\n",
            "Epoch Step: 1100 Loss: 101.135689 Tokens per Sec: 22826.321455\n",
            "Epoch Step: 1200 Loss: 75.424484 Tokens per Sec: 24213.406825\n",
            "Epoch Step: 1300 Loss: 32.518250 Tokens per Sec: 23515.691184\n",
            "Epoch Step: 1400 Loss: 54.139641 Tokens per Sec: 23669.818074\n",
            "Epoch Step: 1500 Loss: 97.672234 Tokens per Sec: 24424.023697\n",
            "Epoch Step: 1600 Loss: 72.154877 Tokens per Sec: 23644.777560\n",
            "Epoch Step: 1700 Loss: 15.400912 Tokens per Sec: 23308.102822\n",
            "Epoch Step: 1800 Loss: 42.712658 Tokens per Sec: 24224.380253\n",
            "Epoch Step: 1900 Loss: 60.062759 Tokens per Sec: 22226.752014\n",
            "Epoch Step: 2000 Loss: 42.051517 Tokens per Sec: 21870.444851\n",
            "Epoch Step: 2100 Loss: 26.091944 Tokens per Sec: 24141.575636\n",
            "Epoch Step: 2200 Loss: 97.326996 Tokens per Sec: 24165.175641\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was years old , i was a <unk> of <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father is to be <unk> , the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he 's very much , what was really interesting , which was really interesting , it was the most important .\n",
            "\n",
            "Validation perplexity: 31.993523\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 41.063404 Tokens per Sec: 23261.479225\n",
            "Epoch Step: 200 Loss: 88.496040 Tokens per Sec: 24330.171219\n",
            "Epoch Step: 300 Loss: 47.883820 Tokens per Sec: 23005.243855\n",
            "Epoch Step: 400 Loss: 41.042995 Tokens per Sec: 24629.793852\n",
            "Epoch Step: 500 Loss: 60.816414 Tokens per Sec: 23243.194514\n",
            "Epoch Step: 600 Loss: 70.609032 Tokens per Sec: 22872.907223\n",
            "Epoch Step: 700 Loss: 55.355095 Tokens per Sec: 24377.806140\n",
            "Epoch Step: 800 Loss: 20.728359 Tokens per Sec: 23967.825873\n",
            "Epoch Step: 900 Loss: 12.206519 Tokens per Sec: 24237.682025\n",
            "Epoch Step: 1000 Loss: 57.283482 Tokens per Sec: 23855.188817\n",
            "Epoch Step: 1100 Loss: 59.177753 Tokens per Sec: 24022.183161\n",
            "Epoch Step: 1200 Loss: 74.445450 Tokens per Sec: 23727.106010\n",
            "Epoch Step: 1300 Loss: 11.693275 Tokens per Sec: 24707.356669\n",
            "Epoch Step: 1400 Loss: 53.480118 Tokens per Sec: 23628.114474\n",
            "Epoch Step: 1500 Loss: 77.909866 Tokens per Sec: 23192.767308\n",
            "Epoch Step: 1600 Loss: 47.568256 Tokens per Sec: 24171.982061\n",
            "Epoch Step: 1700 Loss: 86.256157 Tokens per Sec: 22872.117269\n",
            "Epoch Step: 1800 Loss: 28.623055 Tokens per Sec: 24461.545441\n",
            "Epoch Step: 1900 Loss: 63.102844 Tokens per Sec: 24161.367130\n",
            "Epoch Step: 2000 Loss: 87.332321 Tokens per Sec: 24837.367107\n",
            "Epoch Step: 2100 Loss: 17.394506 Tokens per Sec: 24362.255348\n",
            "Epoch Step: 2200 Loss: 81.287888 Tokens per Sec: 23462.811645\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little bit , his <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was very interesting , because it was the most important , because it 's the most important .\n",
            "\n",
            "Validation perplexity: 19.818345\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 29.107117 Tokens per Sec: 21800.731395\n",
            "Epoch Step: 200 Loss: 18.619778 Tokens per Sec: 22895.534464\n",
            "Epoch Step: 300 Loss: 69.817940 Tokens per Sec: 23098.663814\n",
            "Epoch Step: 400 Loss: 55.754353 Tokens per Sec: 22320.529088\n",
            "Epoch Step: 500 Loss: 18.923126 Tokens per Sec: 21830.328894\n",
            "Epoch Step: 600 Loss: 27.572666 Tokens per Sec: 23897.783741\n",
            "Epoch Step: 700 Loss: 54.655594 Tokens per Sec: 23819.539085\n",
            "Epoch Step: 800 Loss: 68.234673 Tokens per Sec: 24643.928332\n",
            "Epoch Step: 900 Loss: 45.751572 Tokens per Sec: 23979.376876\n",
            "Epoch Step: 1000 Loss: 59.643261 Tokens per Sec: 24158.154903\n",
            "Epoch Step: 1100 Loss: 17.396826 Tokens per Sec: 22724.577495\n",
            "Epoch Step: 1200 Loss: 62.428020 Tokens per Sec: 22856.025251\n",
            "Epoch Step: 1300 Loss: 59.117668 Tokens per Sec: 23448.218387\n",
            "Epoch Step: 1400 Loss: 39.873173 Tokens per Sec: 24160.014259\n",
            "Epoch Step: 1500 Loss: 52.650570 Tokens per Sec: 24302.528587\n",
            "Epoch Step: 1600 Loss: 8.838224 Tokens per Sec: 23245.771795\n",
            "Epoch Step: 1700 Loss: 24.345131 Tokens per Sec: 24402.646860\n",
            "Epoch Step: 1800 Loss: 14.214804 Tokens per Sec: 23921.761955\n",
            "Epoch Step: 1900 Loss: 17.225016 Tokens per Sec: 21820.420409\n",
            "Epoch Step: 2000 Loss: 45.538971 Tokens per Sec: 22845.703745\n",
            "Epoch Step: 2100 Loss: 44.782581 Tokens per Sec: 23224.105816\n",
            "Epoch Step: 2200 Loss: 33.574947 Tokens per Sec: 24172.225356\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i 11 11 years old , i was a <unk> from the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad was on his little , the <unk> of the same time .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 15.835611\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 45.038071 Tokens per Sec: 23081.162507\n",
            "Epoch Step: 200 Loss: 38.104240 Tokens per Sec: 24133.742571\n",
            "Epoch Step: 300 Loss: 34.553165 Tokens per Sec: 23270.084096\n",
            "Epoch Step: 400 Loss: 59.353443 Tokens per Sec: 24045.628981\n",
            "Epoch Step: 500 Loss: 22.442331 Tokens per Sec: 22847.299989\n",
            "Epoch Step: 600 Loss: 16.397945 Tokens per Sec: 23209.551418\n",
            "Epoch Step: 700 Loss: 68.896362 Tokens per Sec: 23630.079019\n",
            "Epoch Step: 800 Loss: 45.968338 Tokens per Sec: 23104.749473\n",
            "Epoch Step: 900 Loss: 30.089853 Tokens per Sec: 23311.850505\n",
            "Epoch Step: 1000 Loss: 22.235327 Tokens per Sec: 24651.111457\n",
            "Epoch Step: 1100 Loss: 3.424968 Tokens per Sec: 23144.618245\n",
            "Epoch Step: 1200 Loss: 64.704262 Tokens per Sec: 24229.669317\n",
            "Epoch Step: 1300 Loss: 52.983856 Tokens per Sec: 23630.323069\n",
            "Epoch Step: 1400 Loss: 32.737431 Tokens per Sec: 24634.943787\n",
            "Epoch Step: 1500 Loss: 42.206799 Tokens per Sec: 24356.298775\n",
            "Epoch Step: 1600 Loss: 69.602524 Tokens per Sec: 22031.180068\n",
            "Epoch Step: 1700 Loss: 30.875692 Tokens per Sec: 23164.604089\n",
            "Epoch Step: 1800 Loss: 64.806252 Tokens per Sec: 22643.168435\n",
            "Epoch Step: 1900 Loss: 54.734386 Tokens per Sec: 24685.775555\n",
            "Epoch Step: 2000 Loss: 7.082081 Tokens per Sec: 23222.437498\n",
            "Epoch Step: 2100 Loss: 8.820795 Tokens per Sec: 23951.809678\n",
            "Epoch Step: 2200 Loss: 49.000469 Tokens per Sec: 23510.529235\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the morning of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , the united states of the bbc was the right of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much , because it was pretty much , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 13.547167\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 58.596680 Tokens per Sec: 22167.877402\n",
            "Epoch Step: 200 Loss: 21.205675 Tokens per Sec: 24014.867478\n",
            "Epoch Step: 300 Loss: 65.655525 Tokens per Sec: 24015.877627\n",
            "Epoch Step: 400 Loss: 10.162137 Tokens per Sec: 22066.633420\n",
            "Epoch Step: 500 Loss: 16.835863 Tokens per Sec: 24210.251951\n",
            "Epoch Step: 600 Loss: 13.602905 Tokens per Sec: 23836.975216\n",
            "Epoch Step: 700 Loss: 40.910080 Tokens per Sec: 23187.624044\n",
            "Epoch Step: 800 Loss: 61.774529 Tokens per Sec: 23523.042370\n",
            "Epoch Step: 900 Loss: 51.547749 Tokens per Sec: 23552.437287\n",
            "Epoch Step: 1000 Loss: 44.447628 Tokens per Sec: 21739.562995\n",
            "Epoch Step: 1100 Loss: 26.960278 Tokens per Sec: 22949.455329\n",
            "Epoch Step: 1200 Loss: 49.037140 Tokens per Sec: 24913.427227\n",
            "Epoch Step: 1300 Loss: 38.518993 Tokens per Sec: 23547.028197\n",
            "Epoch Step: 1400 Loss: 8.823228 Tokens per Sec: 23779.466090\n",
            "Epoch Step: 1500 Loss: 12.673170 Tokens per Sec: 22860.846512\n",
            "Epoch Step: 1600 Loss: 64.088745 Tokens per Sec: 21419.751864\n",
            "Epoch Step: 1700 Loss: 26.450243 Tokens per Sec: 21684.958680\n",
            "Epoch Step: 1800 Loss: 26.269415 Tokens per Sec: 24243.603864\n",
            "Epoch Step: 1900 Loss: 11.098892 Tokens per Sec: 23126.761218\n",
            "Epoch Step: 2000 Loss: 12.712128 Tokens per Sec: 24722.963374\n",
            "Epoch Step: 2100 Loss: 16.925749 Tokens per Sec: 24985.219392\n",
            "Epoch Step: 2200 Loss: 35.567543 Tokens per Sec: 22789.620175\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> from the <unk> <unk> <unk> joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , radio radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much , because it was the news of the <unk> .\n",
            "\n",
            "Validation perplexity: 12.620848\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 34.502052 Tokens per Sec: 22117.598844\n",
            "Epoch Step: 200 Loss: 16.286636 Tokens per Sec: 24551.796154\n",
            "Epoch Step: 300 Loss: 53.757969 Tokens per Sec: 23044.495722\n",
            "Epoch Step: 400 Loss: 31.982208 Tokens per Sec: 24609.717265\n",
            "Epoch Step: 500 Loss: 15.251490 Tokens per Sec: 24665.056648\n",
            "Epoch Step: 600 Loss: 48.020298 Tokens per Sec: 23052.417220\n",
            "Epoch Step: 700 Loss: 56.008144 Tokens per Sec: 23785.596083\n",
            "Epoch Step: 800 Loss: 5.983189 Tokens per Sec: 24916.325235\n",
            "Epoch Step: 900 Loss: 13.622049 Tokens per Sec: 24075.960565\n",
            "Epoch Step: 1000 Loss: 58.574863 Tokens per Sec: 24255.426419\n",
            "Epoch Step: 1100 Loss: 49.632584 Tokens per Sec: 23994.258838\n",
            "Epoch Step: 1200 Loss: 9.427354 Tokens per Sec: 23679.082507\n",
            "Epoch Step: 1300 Loss: 41.019627 Tokens per Sec: 24148.323096\n",
            "Epoch Step: 1400 Loss: 34.913147 Tokens per Sec: 22731.075216\n",
            "Epoch Step: 1500 Loss: 6.046276 Tokens per Sec: 22719.454115\n",
            "Epoch Step: 1600 Loss: 46.531822 Tokens per Sec: 23517.725917\n",
            "Epoch Step: 1700 Loss: 23.158430 Tokens per Sec: 24284.402070\n",
            "Epoch Step: 1800 Loss: 23.448538 Tokens per Sec: 23595.594861\n",
            "Epoch Step: 1900 Loss: 18.845385 Tokens per Sec: 23854.536381\n",
            "Epoch Step: 2000 Loss: 57.514656 Tokens per Sec: 24405.129416\n",
            "Epoch Step: 2100 Loss: 45.797993 Tokens per Sec: 23937.502538\n",
            "Epoch Step: 2200 Loss: 58.214699 Tokens per Sec: 23406.848643\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 , i was one of the <unk> of the <unk> <unk> joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on the little , the radio <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was going to be the news .\n",
            "\n",
            "Validation perplexity: 12.114621\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 40.868366 Tokens per Sec: 23087.964352\n",
            "Epoch Step: 200 Loss: 40.302097 Tokens per Sec: 24424.500243\n",
            "Epoch Step: 300 Loss: 54.446892 Tokens per Sec: 24698.301138\n",
            "Epoch Step: 400 Loss: 51.983036 Tokens per Sec: 24444.246976\n",
            "Epoch Step: 500 Loss: 10.388141 Tokens per Sec: 24556.567552\n",
            "Epoch Step: 600 Loss: 27.065104 Tokens per Sec: 24329.218311\n",
            "Epoch Step: 700 Loss: 31.456251 Tokens per Sec: 24882.412029\n",
            "Epoch Step: 800 Loss: 20.793428 Tokens per Sec: 24355.239064\n",
            "Epoch Step: 900 Loss: 38.468410 Tokens per Sec: 23300.959962\n",
            "Epoch Step: 1000 Loss: 47.425636 Tokens per Sec: 23076.116821\n",
            "Epoch Step: 1100 Loss: 45.043530 Tokens per Sec: 24610.959642\n",
            "Epoch Step: 1200 Loss: 29.888729 Tokens per Sec: 24354.801129\n",
            "Epoch Step: 1300 Loss: 17.518677 Tokens per Sec: 23419.665321\n",
            "Epoch Step: 1400 Loss: 25.303537 Tokens per Sec: 24678.886486\n",
            "Epoch Step: 1500 Loss: 22.242544 Tokens per Sec: 24109.586483\n",
            "Epoch Step: 1600 Loss: 41.364754 Tokens per Sec: 24339.751888\n",
            "Epoch Step: 1700 Loss: 38.794178 Tokens per Sec: 24514.132436\n",
            "Epoch Step: 1800 Loss: 30.151329 Tokens per Sec: 24617.104221\n",
            "Epoch Step: 1900 Loss: 15.674362 Tokens per Sec: 24440.947283\n",
            "Epoch Step: 2000 Loss: 54.026066 Tokens per Sec: 23447.267776\n",
            "Epoch Step: 2100 Loss: 43.388569 Tokens per Sec: 23721.369475\n",
            "Epoch Step: 2200 Loss: 31.290127 Tokens per Sec: 22414.401180\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , radio waves the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was going to go out of the news .\n",
            "\n",
            "Validation perplexity: 11.833294\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 31.749214 Tokens per Sec: 23275.407206\n",
            "Epoch Step: 200 Loss: 50.014252 Tokens per Sec: 24982.943702\n",
            "Epoch Step: 300 Loss: 27.513046 Tokens per Sec: 22837.131321\n",
            "Epoch Step: 400 Loss: 19.247433 Tokens per Sec: 23573.720239\n",
            "Epoch Step: 500 Loss: 33.596306 Tokens per Sec: 23102.873074\n",
            "Epoch Step: 600 Loss: 35.156681 Tokens per Sec: 24564.160259\n",
            "Epoch Step: 700 Loss: 6.282680 Tokens per Sec: 25013.836536\n",
            "Epoch Step: 800 Loss: 29.781038 Tokens per Sec: 23715.927683\n",
            "Epoch Step: 900 Loss: 57.956177 Tokens per Sec: 22811.692725\n",
            "Epoch Step: 1000 Loss: 9.533410 Tokens per Sec: 24768.303795\n",
            "Epoch Step: 1100 Loss: 47.938953 Tokens per Sec: 24574.720407\n",
            "Epoch Step: 1200 Loss: 14.981913 Tokens per Sec: 24868.107955\n",
            "Epoch Step: 1300 Loss: 20.902565 Tokens per Sec: 23527.252336\n",
            "Epoch Step: 1400 Loss: 25.835709 Tokens per Sec: 24523.019751\n",
            "Epoch Step: 1500 Loss: 45.077133 Tokens per Sec: 24230.676686\n",
            "Epoch Step: 1600 Loss: 24.879620 Tokens per Sec: 24545.845582\n",
            "Epoch Step: 1700 Loss: 31.355001 Tokens per Sec: 23228.975479\n",
            "Epoch Step: 1800 Loss: 31.862806 Tokens per Sec: 22903.071016\n",
            "Epoch Step: 1900 Loss: 18.863791 Tokens per Sec: 23149.887153\n",
            "Epoch Step: 2000 Loss: 5.610421 Tokens per Sec: 24617.351580\n",
            "Epoch Step: 2100 Loss: 42.618214 Tokens per Sec: 23794.115073\n",
            "Epoch Step: 2200 Loss: 19.178226 Tokens per Sec: 24710.143776\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was going to go out of the news .\n",
            "\n",
            "Validation perplexity: 11.703359\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 11.306583 Tokens per Sec: 22613.247404\n",
            "Epoch Step: 200 Loss: 13.802611 Tokens per Sec: 24270.910658\n",
            "Epoch Step: 300 Loss: 28.512735 Tokens per Sec: 23674.857966\n",
            "Epoch Step: 400 Loss: 9.145544 Tokens per Sec: 22116.815346\n",
            "Epoch Step: 500 Loss: 18.074389 Tokens per Sec: 23497.332400\n",
            "Epoch Step: 600 Loss: 8.643480 Tokens per Sec: 22913.878231\n",
            "Epoch Step: 700 Loss: 54.182201 Tokens per Sec: 24188.248044\n",
            "Epoch Step: 800 Loss: 24.561501 Tokens per Sec: 23689.905018\n",
            "Epoch Step: 900 Loss: 37.876217 Tokens per Sec: 23283.696208\n",
            "Epoch Step: 1000 Loss: 20.057590 Tokens per Sec: 22604.131441\n",
            "Epoch Step: 1100 Loss: 15.861329 Tokens per Sec: 23332.855105\n",
            "Epoch Step: 1200 Loss: 24.589373 Tokens per Sec: 24041.373139\n",
            "Epoch Step: 1300 Loss: 35.553608 Tokens per Sec: 24846.096503\n",
            "Epoch Step: 1400 Loss: 28.055149 Tokens per Sec: 22705.063483\n",
            "Epoch Step: 1500 Loss: 4.820250 Tokens per Sec: 22234.043740\n",
            "Epoch Step: 1600 Loss: 25.008366 Tokens per Sec: 23130.707363\n",
            "Epoch Step: 1700 Loss: 4.098173 Tokens per Sec: 23779.329135\n",
            "Epoch Step: 1800 Loss: 18.287582 Tokens per Sec: 23452.312258\n",
            "Epoch Step: 1900 Loss: 30.403728 Tokens per Sec: 23777.148899\n",
            "Epoch Step: 2000 Loss: 47.743519 Tokens per Sec: 23802.561712\n",
            "Epoch Step: 2100 Loss: 27.201971 Tokens per Sec: 24838.433873\n",
            "Epoch Step: 2200 Loss: 53.260822 Tokens per Sec: 24059.729174\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> <unk> delight .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , gray radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was going to go out there because it was the news .\n",
            "\n",
            "Validation perplexity: 11.730503\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 8.304938 Tokens per Sec: 22438.646451\n",
            "Epoch Step: 200 Loss: 34.588314 Tokens per Sec: 23678.624746\n",
            "Epoch Step: 300 Loss: 33.480522 Tokens per Sec: 24651.324881\n",
            "Epoch Step: 400 Loss: 28.250845 Tokens per Sec: 24582.991612\n",
            "Epoch Step: 500 Loss: 25.022881 Tokens per Sec: 22922.968772\n",
            "Epoch Step: 600 Loss: 16.460705 Tokens per Sec: 22409.972606\n",
            "Epoch Step: 700 Loss: 44.365288 Tokens per Sec: 23747.740509\n",
            "Epoch Step: 800 Loss: 16.001741 Tokens per Sec: 23157.963745\n",
            "Epoch Step: 900 Loss: 46.882557 Tokens per Sec: 23697.834216\n",
            "Epoch Step: 1000 Loss: 24.498060 Tokens per Sec: 24626.483559\n",
            "Epoch Step: 1100 Loss: 18.598452 Tokens per Sec: 24551.415652\n",
            "Epoch Step: 1200 Loss: 21.931870 Tokens per Sec: 23744.752604\n",
            "Epoch Step: 1300 Loss: 18.914234 Tokens per Sec: 23550.180138\n",
            "Epoch Step: 1400 Loss: 26.156668 Tokens per Sec: 23202.366165\n",
            "Epoch Step: 1500 Loss: 13.708686 Tokens per Sec: 24530.000902\n",
            "Epoch Step: 1600 Loss: 32.667416 Tokens per Sec: 22834.491043\n",
            "Epoch Step: 1700 Loss: 21.964394 Tokens per Sec: 22462.290066\n",
            "Epoch Step: 1800 Loss: 46.444935 Tokens per Sec: 23984.909672\n",
            "Epoch Step: 1900 Loss: 1.677988 Tokens per Sec: 24014.418717\n",
            "Epoch Step: 2000 Loss: 24.408318 Tokens per Sec: 24626.888577\n",
            "Epoch Step: 2100 Loss: 11.212330 Tokens per Sec: 24607.334278\n",
            "Epoch Step: 2200 Loss: 24.969936 Tokens per Sec: 23818.271956\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of <unk> <unk> from the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , radio radio waves of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was going to go out the news .\n",
            "\n",
            "Validation perplexity: 11.944606\n",
            "Epoch 10\n",
            "Epoch Step: 100 Loss: 34.608521 Tokens per Sec: 20884.817559\n",
            "Epoch Step: 200 Loss: 32.612785 Tokens per Sec: 24474.491377\n",
            "Epoch Step: 300 Loss: 46.186218 Tokens per Sec: 24276.940728\n",
            "Epoch Step: 400 Loss: 18.421822 Tokens per Sec: 24289.741740\n",
            "Epoch Step: 500 Loss: 15.464917 Tokens per Sec: 22209.663768\n",
            "Epoch Step: 600 Loss: 26.245510 Tokens per Sec: 23198.722185\n",
            "Epoch Step: 700 Loss: 4.089848 Tokens per Sec: 23687.593803\n",
            "Epoch Step: 800 Loss: 42.429260 Tokens per Sec: 23649.693198\n",
            "Epoch Step: 900 Loss: 5.107025 Tokens per Sec: 22039.676098\n",
            "Epoch Step: 1000 Loss: 13.792211 Tokens per Sec: 23794.526091\n",
            "Epoch Step: 1100 Loss: 48.232777 Tokens per Sec: 22496.610515\n",
            "Epoch Step: 1200 Loss: 34.667126 Tokens per Sec: 23564.150166\n",
            "Epoch Step: 1300 Loss: 21.877911 Tokens per Sec: 23261.007552\n",
            "Epoch Step: 1400 Loss: 47.757050 Tokens per Sec: 24190.171959\n",
            "Epoch Step: 1500 Loss: 16.154940 Tokens per Sec: 23276.964740\n",
            "Epoch Step: 1600 Loss: 32.676659 Tokens per Sec: 23835.742465\n",
            "Epoch Step: 1700 Loss: 5.687030 Tokens per Sec: 24774.604330\n",
            "Epoch Step: 1800 Loss: 24.952974 Tokens per Sec: 24304.889095\n",
            "Epoch Step: 1900 Loss: 45.361729 Tokens per Sec: 24632.409680\n",
            "Epoch Step: 2000 Loss: 16.835629 Tokens per Sec: 22776.880128\n",
            "Epoch Step: 2100 Loss: 51.961391 Tokens per Sec: 24836.764452\n",
            "Epoch Step: 2200 Loss: 14.909551 Tokens per Sec: 23995.023366\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a doctor in the morning of the pleasure of joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , radio shack the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was quite unusual , because it was the news , because it was the news of the <unk> .\n",
            "\n",
            "Validation perplexity: 12.112863\n",
            "Epoch 11\n",
            "Epoch Step: 100 Loss: 43.456146 Tokens per Sec: 22706.205622\n",
            "Epoch Step: 200 Loss: 39.180916 Tokens per Sec: 23635.595069\n",
            "Epoch Step: 300 Loss: 9.531148 Tokens per Sec: 23121.526247\n",
            "Epoch Step: 400 Loss: 35.393982 Tokens per Sec: 22779.785528\n",
            "Epoch Step: 500 Loss: 20.604725 Tokens per Sec: 23653.060390\n",
            "Epoch Step: 600 Loss: 29.413582 Tokens per Sec: 24702.393369\n",
            "Epoch Step: 700 Loss: 33.679058 Tokens per Sec: 24737.926439\n",
            "Epoch Step: 800 Loss: 23.039070 Tokens per Sec: 23781.386264\n",
            "Epoch Step: 900 Loss: 16.535208 Tokens per Sec: 24049.424995\n",
            "Epoch Step: 1000 Loss: 20.004745 Tokens per Sec: 22909.385071\n",
            "Epoch Step: 1100 Loss: 28.974892 Tokens per Sec: 21886.270555\n",
            "Epoch Step: 1200 Loss: 34.496597 Tokens per Sec: 24843.852839\n",
            "Epoch Step: 1300 Loss: 24.333078 Tokens per Sec: 23364.546155\n",
            "Epoch Step: 1400 Loss: 12.874847 Tokens per Sec: 23640.476774\n",
            "Epoch Step: 1500 Loss: 40.031055 Tokens per Sec: 24136.995268\n",
            "Epoch Step: 1600 Loss: 12.227449 Tokens per Sec: 23227.341107\n",
            "Epoch Step: 1700 Loss: 24.827103 Tokens per Sec: 23809.857716\n",
            "Epoch Step: 1800 Loss: 14.974076 Tokens per Sec: 24553.081500\n",
            "Epoch Step: 1900 Loss: 20.603598 Tokens per Sec: 23402.162917\n",
            "Epoch Step: 2000 Loss: 30.891680 Tokens per Sec: 22829.256803\n",
            "Epoch Step: 2100 Loss: 15.550178 Tokens per Sec: 23783.866228\n",
            "Epoch Step: 2200 Loss: 11.359536 Tokens per Sec: 22281.038803\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a doctor of the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , tiny radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 12.491657\n",
            "Epoch 12\n",
            "Epoch Step: 100 Loss: 20.783337 Tokens per Sec: 23681.115198\n",
            "Epoch Step: 200 Loss: 5.895877 Tokens per Sec: 24666.481563\n",
            "Epoch Step: 300 Loss: 39.518085 Tokens per Sec: 24032.396674\n",
            "Epoch Step: 400 Loss: 9.735713 Tokens per Sec: 23375.571376\n",
            "Epoch Step: 500 Loss: 16.988922 Tokens per Sec: 24650.386899\n",
            "Epoch Step: 600 Loss: 24.419081 Tokens per Sec: 24890.337801\n",
            "Epoch Step: 700 Loss: 31.211422 Tokens per Sec: 23222.121779\n",
            "Epoch Step: 800 Loss: 33.465633 Tokens per Sec: 24582.831576\n",
            "Epoch Step: 900 Loss: 16.076532 Tokens per Sec: 24009.688571\n",
            "Epoch Step: 1000 Loss: 36.508411 Tokens per Sec: 22120.935687\n",
            "Epoch Step: 1100 Loss: 18.087723 Tokens per Sec: 21829.219081\n",
            "Epoch Step: 1200 Loss: 16.635399 Tokens per Sec: 24664.901368\n",
            "Epoch Step: 1300 Loss: 18.376396 Tokens per Sec: 23622.597978\n",
            "Epoch Step: 1400 Loss: 11.393141 Tokens per Sec: 24632.154086\n",
            "Epoch Step: 1500 Loss: 15.182117 Tokens per Sec: 24112.291909\n",
            "Epoch Step: 1600 Loss: 24.437962 Tokens per Sec: 23910.182138\n",
            "Epoch Step: 1700 Loss: 9.578748 Tokens per Sec: 21752.551554\n",
            "Epoch Step: 1800 Loss: 17.114864 Tokens per Sec: 22698.848060\n",
            "Epoch Step: 1900 Loss: 8.581913 Tokens per Sec: 23257.589590\n",
            "Epoch Step: 2000 Loss: 25.176817 Tokens per Sec: 23936.300824\n",
            "Epoch Step: 2100 Loss: 29.144550 Tokens per Sec: 24446.446758\n",
            "Epoch Step: 2200 Loss: 19.318604 Tokens per Sec: 24313.186786\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was one of the dawn of the <unk> <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , bang , the radio <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite a while , because the news was <unk> the news .\n",
            "\n",
            "Validation perplexity: 12.577149\n",
            "Epoch 13\n",
            "Epoch Step: 100 Loss: 15.206842 Tokens per Sec: 22709.924754\n",
            "Epoch Step: 200 Loss: 28.971737 Tokens per Sec: 22994.901816\n",
            "Epoch Step: 300 Loss: 10.210945 Tokens per Sec: 23573.319523\n",
            "Epoch Step: 400 Loss: 38.240730 Tokens per Sec: 23691.365634\n",
            "Epoch Step: 500 Loss: 43.327106 Tokens per Sec: 24953.152456\n",
            "Epoch Step: 600 Loss: 22.038826 Tokens per Sec: 23383.535927\n",
            "Epoch Step: 700 Loss: 20.369247 Tokens per Sec: 23731.391551\n",
            "Epoch Step: 800 Loss: 28.249914 Tokens per Sec: 23449.866087\n",
            "Epoch Step: 900 Loss: 23.989212 Tokens per Sec: 24891.810677\n",
            "Epoch Step: 1000 Loss: 32.201294 Tokens per Sec: 22614.251711\n",
            "Epoch Step: 1100 Loss: 32.367764 Tokens per Sec: 23447.102041\n",
            "Epoch Step: 1200 Loss: 27.906656 Tokens per Sec: 23027.578970\n",
            "Epoch Step: 1300 Loss: 45.800556 Tokens per Sec: 24136.126734\n",
            "Epoch Step: 1400 Loss: 31.421515 Tokens per Sec: 22805.118566\n",
            "Epoch Step: 1500 Loss: 17.511604 Tokens per Sec: 23444.680706\n",
            "Epoch Step: 1600 Loss: 34.965847 Tokens per Sec: 24718.557641\n",
            "Epoch Step: 1700 Loss: 23.463377 Tokens per Sec: 23463.946700\n",
            "Epoch Step: 1800 Loss: 38.796246 Tokens per Sec: 24630.400512\n",
            "Epoch Step: 1900 Loss: 25.974243 Tokens per Sec: 24901.170472\n",
            "Epoch Step: 2000 Loss: 33.878124 Tokens per Sec: 24560.765999\n",
            "Epoch Step: 2100 Loss: 24.731165 Tokens per Sec: 24711.980045\n",
            "Epoch Step: 2200 Loss: 26.628174 Tokens per Sec: 24188.040416\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of a <unk> in the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped listening on the little , radio radio <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite unusual , because it was the news , because it was the news .\n",
            "\n",
            "Validation perplexity: 12.977343\n",
            "Epoch 14\n",
            "Epoch Step: 100 Loss: 24.567619 Tokens per Sec: 23115.545877\n",
            "Epoch Step: 200 Loss: 6.317229 Tokens per Sec: 24462.431461\n",
            "Epoch Step: 300 Loss: 11.907596 Tokens per Sec: 22267.932924\n",
            "Epoch Step: 400 Loss: 40.233768 Tokens per Sec: 24051.211439\n",
            "Epoch Step: 500 Loss: 11.980370 Tokens per Sec: 24705.386726\n",
            "Epoch Step: 600 Loss: 24.866961 Tokens per Sec: 24098.411431\n",
            "Epoch Step: 700 Loss: 13.523805 Tokens per Sec: 23255.888095\n",
            "Epoch Step: 800 Loss: 18.000912 Tokens per Sec: 24702.525302\n",
            "Epoch Step: 900 Loss: 32.793507 Tokens per Sec: 24132.837013\n",
            "Epoch Step: 1000 Loss: 16.613806 Tokens per Sec: 24741.289490\n",
            "Epoch Step: 1100 Loss: 39.618969 Tokens per Sec: 24230.094899\n",
            "Epoch Step: 1200 Loss: 16.028519 Tokens per Sec: 22494.050224\n",
            "Epoch Step: 1300 Loss: 4.998873 Tokens per Sec: 24787.961451\n",
            "Epoch Step: 1400 Loss: 24.106701 Tokens per Sec: 24257.670916\n",
            "Epoch Step: 1500 Loss: 22.579931 Tokens per Sec: 24511.239815\n",
            "Epoch Step: 1600 Loss: 34.454021 Tokens per Sec: 25004.893425\n",
            "Epoch Step: 1700 Loss: 11.332912 Tokens per Sec: 24092.283850\n",
            "Epoch Step: 1800 Loss: 45.456593 Tokens per Sec: 24329.844904\n",
            "Epoch Step: 1900 Loss: 28.725155 Tokens per Sec: 22801.109583\n",
            "Epoch Step: 2000 Loss: 39.364731 Tokens per Sec: 23781.575990\n",
            "Epoch Step: 2100 Loss: 4.615611 Tokens per Sec: 23237.405875\n",
            "Epoch Step: 2200 Loss: 37.232090 Tokens per Sec: 23345.853674\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a doctor of the <unk> <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on the little bit of radio , radio radio <unk> the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , and then , at the time , there was quite a time , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 13.355899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6UDES4bU8-k",
        "outputId": "683c6fb0-b352-45b3-eebb-c8277871a21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKymiMQvU8-p"
      },
      "source": [
        "## Prediction and Evaluation\n",
        "\n",
        "Once trained we can use the model to produce a set of translations. \n",
        "\n",
        "If we translate the whole validation set, we can use [SacreBLEU](https://github.com/mjpost/sacreBLEU) to get a [BLEU score](https://en.wikipedia.org/wiki/BLEU), which is the most common way to evaluate translations.\n",
        "\n",
        "#### Important sidenote\n",
        "Typically you would use SacreBLEU from the **command line** using the output file and original (possibly tokenized) development reference file. This will give you a nice version string that shows how the BLEU score was calculated; for example, if it was lowercased, if it was tokenized (and how), and what smoothing was used. If you want to learn more about how BLEU scores are (and should be) reported, check out [this paper](https://arxiv.org/abs/1804.08771).\n",
        "\n",
        "However, right now our pre-processed data is only in memory, so we'll calculate the BLEU score right from this notebook for demonstration purposes.\n",
        "\n",
        "We'll first test the raw BLEU function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnOoL3ojU8-p"
      },
      "source": [
        "import sacrebleu"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc0NO1a5U8-s",
        "outputId": "d408357f-2c2f-480c-d2e1-d16d43402b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this should result in a perfect BLEU of 100%\n",
        "hypotheses = [\"this is a test\"]\n",
        "references = [\"this is a test\"]\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.00000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrLQzH5VU8-w",
        "outputId": "f597b6df-bcf8-4d40-adfb-b28468f92a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# here the BLEU score will be lower, because some n-grams won't match\n",
        "hypotheses = [\"this is a test\"]\n",
        "references = [\"this is a fest\"]\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.360679774997894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FFcZJRNU8-0"
      },
      "source": [
        "Since we did some filtering for speed, our validation set contains 690 sentences.\n",
        "The references are the tokenized versions, but they should not contain out-of-vocabulary UNKs that our network might have seen. So we'll take the references straight out of the `valid_data` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlUiOPWOU8-0",
        "outputId": "4b4e8e97-d325-4d8a-e160-cba01b8de1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(valid_data)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDMy76M-U8-3",
        "outputId": "87d97b2f-5a20-4dee-aa49-c12636e0ef93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "references = [\" \".join(example.trg) for example in valid_data]\n",
        "print(len(references))\n",
        "print(references[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 , i remember waking up one morning to the sound of joy in my house .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neyttJQXU8-5",
        "outputId": "3abda038-23e5-4ad9-b051-2e46d5b13565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "references[-2]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i 'm always the one taking the picture .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYyFIG6FU8-8"
      },
      "source": [
        "**Now we translate the validation set!**\n",
        "\n",
        "This might take a little bit of time.\n",
        "\n",
        "Note that `greedy_decode` will cut-off the sentence when it encounters the end-of-sequence symbol, if we provide it the index of that symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_H6zE1zU8-9"
      },
      "source": [
        "hypotheses = []\n",
        "alphas = []  # save the last attention scores\n",
        "for batch in valid_iter:\n",
        "  print(\"batch   :\", batch.src)\n",
        "  batch = rebatch(PAD_INDEX, batch)\n",
        "  print(\"batch2   :\" , batch.src)\n",
        "  pred, attention = greedy_decode(\n",
        "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
        "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "  hypotheses.append(pred)\n",
        "  alphas.append(attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUJ1SeWU8_C",
        "outputId": "ded8ecbb-74be-4877-a821-0b32453a48fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we will still need to convert the indices to actual words!\n",
        "hypotheses[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  70,   11,   24, 1460, 1460,    5,   11,   24,    9,    0,   16,\n",
              "          6,  690,   10,    6, 1806,    4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoW9yR-DU8_G",
        "outputId": "e5c6e15a-fc43-44c3-f03f-021488027c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
        "hypotheses[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['when',\n",
              " 'i',\n",
              " 'was',\n",
              " '11',\n",
              " '11',\n",
              " ',',\n",
              " 'i',\n",
              " 'was',\n",
              " 'a',\n",
              " '<unk>',\n",
              " 'in',\n",
              " 'the',\n",
              " 'morning',\n",
              " 'of',\n",
              " 'the',\n",
              " 'joy',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__AMwXXIU8_K",
        "outputId": "a467ffd3-87ff-4817-b1b0-d298a3a1104a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
        "hypotheses = [\" \".join(x) for x in hypotheses]\n",
        "print(len(hypotheses))\n",
        "print(hypotheses[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 11 , i was a <unk> in the morning of the joy .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ced__9DNU8_N",
        "outputId": "66f9b8fc-db4a-4ffd-ea93-8145fab3c0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# now we can compute the BLEU score!\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.374138837351047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4e-A3C6U8_Q"
      },
      "source": [
        "## Attention Visualization\n",
        "\n",
        "We can also visualize the attention scores of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md3I2ZqSU8_R"
      },
      "source": [
        "def plot_heatmap(src, trg, scores):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
        "\n",
        "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
        "    ax.set_yticklabels(src, minor=False)\n",
        "\n",
        "    # put the major ticks at the middle of each cell\n",
        "    # and the x-ticks on top\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)\n",
        "    plt.show()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZVNhQO9U8_V",
        "outputId": "02558728-2447-45ed-aaac-e6c59f1de9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "# This plots a chosen sentence, for which we saved the attention scores above.\n",
        "idx = 5\n",
        "src = valid_data[idx].src + [\"</s>\"]\n",
        "trg = valid_data[idx].trg + [\"</s>\"]\n",
        "pred = hypotheses[idx].split() + [\"</s>\"]\n",
        "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
        "print(\"src\", src)\n",
        "print(\"ref\", trg)\n",
        "print(\"pred\", pred)\n",
        "plot_heatmap(src, pred, pred_att)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src ['\"', 'jetzt', 'kannst', 'du', 'auf', 'eine', 'richtige', 'schule', 'gehen', ',', '\"', 'sagte', 'er', '.', '</s>']\n",
            "ref ['\"', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '\"', 'he', 'said', '.', '</s>']\n",
            "pred ['\"', 'now', 'you', 'can', 'go', 'to', 'a', 'real', 'time', ',', '\"', 'he', 'said', '.', '</s>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEHCAYAAABLKzaMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVbnu8d+TMAQIowwyRzEIiBA1wFFBRQVRUI4CgsL14IR6nI4eOQevXEScAb3X8VzQo+B0AREUkUkZRJEhgYQpiiLzIAJhRkLS/dw/9m5SNN1dVV27e+9Unu/nsz9dtWvvt1Z3Om+vWnutd8s2ERHRn6bU3YCIiJg4SfIREX0sST4ioo8lyUdE9LEk+YiIPpYkHxHRx5LkIyL6WJJ8REQfS5KPiL4j6dmSVHc7miBJPiL6iqS1gZuAN9XdliZIko+IfnMg8GvgPXU3pAmS5CPGIGlLSedLuq58vp2kw+tuV4zpncCHgE0lbVh3Y+qWJN8lSdPqbkNMqu8AnwQWA9i+Bjig1hbFqCTNBu6zfTvwA+DgeltUvxXqbsAy6DpJ9wC/K7ff236o5jbFxFnV9hXDruEtqasxTSHpl8CoJWxt1zUe/m7gv8vHPwR+C3yxprY0QpJ8l2w/T9JmwC7AnsC3JD1oe1bNTYuSpHXGet32wi7C3SdpC8qEJmlf4O4emtcvji2/vgV4NvCj8vnbgHvqaJCkVYE9gI8A2L5X0g2SXmX7ojra1ARKPfnuSNqEIsG/EtgeWEjRm++4tyDpHSPtt/2DcbRnKrABLX+wbd/WbZx+IulmiqQ80hQ6235uF7GeCxwPvAx4ALgZOMj2LRU0dZknaa7t2e32TVJbVgTWtv33ln1rANh+eLLb0xTpyXfvNmAO8AXb7x9njB1aHk8DXgNcRTGG2DFJHwY+TdFzGix3G9hunO3qC7afU2Gsm4DXSloNmGL7kapi94nVJD23/Dkh6TnAanU0xPZiSY9JmmJ7UNKWwFbA2XW0pynSk++SpO2BnYFXAJsBfwF+a/u/xzxx7JhrASfZ3qPL824EdrJ9/3jfu9+Vc6ZnUvwxBcD2xV2cvxbwDmAGT/+09JHqWrnskrQHxSedmyg+OW0OvM/2uTW150qKT9prA5dQdMietH1gHe1pgvTku2T7akl/Bf5K8ct0EMXQzbiTPPAYMJ7e5+1ALvqOQtJ7gI8CmwDzgX8CLgVe3UWYs4DLgGtZ+mkpSrbPkTSToscM8Cfbi2pskmw/LundwLdtHy1pfo3tqV2SfJckzQVWBv5AMbvmFbZv7TJG68yEqcDWwCnjaM5NwEWSfgU89R/L9lfHEasffZRiaOwy27tK2gr4Qpcxptn+ePVNW7ZJerXtCyS9ZdhLW0jC9mm1NAwk6aUUC6LeXe6bWlNbGiFJvnuvt31vjzGObXm8BLjV9h3jiHNbua1UbvF0T9h+QhKSVrb9J0nP7zLGDyW9FziTp/8h7WaGTj96JXAB8MYRXjNQV5L/N4p1Dafbvr68cH5hTW1phIzJd0nSmhQXO19R7votcFS3c+UlbcDSC7BXtM4ImGzl93QkxfATjPN7ahpJp1Osfvw3iiGaB4AVbb+hixgfBD4PPMjST19dzdBpqvLC5H8BG9jeVtJ2wJtsf67mpnVN0ieBc2zPq7stTZMk3yVJPwOuA04sd/0PYHvbwz+2jhXjrcAxwEUUF6t2AQ61fWqXbVkP+A/gBTz9wmI3Y86VfE9NJ+mVwJoUieDJLs67CdjR9n0T1riaSPotcChwnO0Xlfuus71tl3H25Jm/g0dV2dYO2rA/8HqKac1XU8yoOc/2A5PZjibKcE33trC9T8vzz4zjws6ngB2Geu9lsv4N0FWSB34MnAzsBbwf+BdgPENJVXxPQLM+oZTt2RmYafv75c95Y4q57p26EXh8QhpXv55X80r6v8CqwK7Ad4F9gSsqa2GHbJ9M8X8BSS+iWBR1WrmO5DcUf9wnvV1NkNo13ftHmTgAkPRy4B9dxpgyLPndz/j+LZ5VTt1cbPu3tt9FdzNHhlTxPQ19QrkC2A94K3B5uUK0FpI+DfwnxRgtwIosXZnZqceA+ZKOk/T1oa3KdtaoitW8L7P9DuAB258BXgpsWW0zu2N7nu0v2t6VogN0PctxRcr05Lv3AeDEchwbinHef+kyxtmSzgX+X/l8f4qpet1aXH69u/zIfBcw5pL+Ubwf+EGP3xNU9wmlKm8GXkSx0Azbd0lavcsYPy+3fvRBijnuW0m6k+ITTrfzyZ8ovz4uaSOKFeC1VH4syxrMtH11y+61KGZX/ayONjVBknz3/ggcDWxB8Qv0EPDPwDVdxLiDYr720IXO422fPo62fK5MzP8OfANYg+IiY7deQzEeP718/iiwQ7lysJthm6o+ofS8iKn0pG1LGuqpdr0S0/aJ7Y9aZt0JfJ9i9sk6wMMUf9y7GU//Zblg7BiKP6amqNxZh8UUQzTb2X6s3Pdd4H9SfK/LpST57v2CYqbFVYz/F2d9iiJKVwHfA8a7OnA/iro51wG7loW5jgV+2WWc2eV2BsWF4AMp/mi9X9JPbR/dYZxKPqFUsYhJxUDzmZKOA9Yqp0G+iw4TkKRTbL9V0rU8s9qibW/faVsarPV3+a5xxvgTMGD7Z5K2AV5MTZ98yrIGp1MMFX6/LCS4nu25dbSnMWxn62IDrqsojoDXASdRXNz7AsUF0G5izOtkXwdxLgamtzyfTjGNchVgQRdxvkxRlfCr5fZm4MvjaM+1FD34+eXzrYDTxhlnN4pe5rHAbl2cu2H59RSKpfpD2wzglLp/Dyv6Hez5dxm4pvy6M8Ungj2By2v8nrYCLi4fHw58pO6fc93bctGTl3QhRW9soe1eLwT+QdILbV/bSxDblvQ34G8UMxrWBk6V9Gvb/9FhmCmS1nY5TazsyY/n33R9Whb6UHzs3cD2PyR1s0R9N9v/SctCGEmfobj42Y0qFjFB0UN90Pah3Z5oe+gC5PM8bEVzuXK2ZyruWrTQXZQBaODv8kD5dU/gO7Z/Jam2efbl74rKNQAHsHRIdLm1XCR5irvDmKW/kL3YGThYRTnbRRQ9ctvuuPKjpI9SFL26j2LM8FAXHzWnUBQ86zTJfwW4VNJPy+f7USzc6daPKWbC/KJ8/kbgJ+UY9oJ2J0v6APCvwHMltV6bWJ2iSFS37ijHeX8O/FrSA0BXpSNKOwEHSrqVYpYMAJ38W03A9zSSH1KUAfiZ7U90eM7B9Pi73DIEtQLwznItwLh+l4E7yyGx3YAvS1qZCmftSXq27b91edp/U/y/utaZJ798LIbS0vri99reqcdYm4+0f3hvr02MzwDfG+kcSVvb/mMXsbZh6Vj1BbbbJuVR4swGXl4+vcRdjGOWF3/XprgDz2EtLz3iHpf/j3cRU3nuuP+tJvJ7GvY+AraxfX2Hx/f8uzzaz2VIl7/LQzfquNb2X8pPJy+0fd542jZC/F/Z3rPLc1almAq6j+3fVNGOZdlykeQjIpZXWQwVEdHHlsskL+mQxFk24jSpLYkzOXGa1JZ+sFwmeaCqf/zEmfg4TWpL4kxOnCa1ZZm3vCb5iIjlQt9deF1znRW8/sYrjnnMQwsHWHOdsW8Wc/vCddu+18BjjzF1tbFXyq90d/sChov9BCtq2ugHdPhvtJhFrMjKHR27rMRpUlsSZ3LiTGZbHuGB+2yv18v7vG7X1Xz/ws5mtF55zaJz3eW9nHvVd/Pk1994Rb52xhY9x/nYj97d/qAObP7ZOT3H8JLF7Q+KiK79xqeOZ/3F09y/cIArzt2so2OnbviX9r3HivVdko+ImEwGBht8j/ck+YiIHhiz2FUspp8YSfIRET1qck9+mZldI+kWSTMkXVR3WyIihhgz4M62OqQnHxHRo8Fn3HKgOZalJH8vReW9yopDRUT0qigJmiTfM9s7lA/fMvy1cvnyIQDrbTT2HPmIiKqlJz/BbB9PcUNiZr5wleb+tCOi7xhY3OBFpX2R5CMi6mKc4ZqIiL5lGGhujk+Sj4joRbHitbmS5CMieiIGUN2NGFXfJfm/37gWX99r757jrP7Saj5/Df7TC3qOMfWqP1fQEvCiRdXEGahoCbcqWovnZvWjpk6f3nOMgUcfraAlMRmKC69J8hERfamYJ58kHxHRtwbTk4+I6E/pyUdE9DEjBhpc63FSWibpD2O8tpakf21zfttjIiLqMmh1tNVhUpK87ZeN8fJaQLsE3skxERGTzognPbWjrQ6TMlwj6VHb0yUdCrwVWBk43fangS8BW0iaD/wa+AfwpvLU9YDzgFVaj7F96GS0OyKinWIxVHOHayZtTF7S7sBMYEdAwBmSXgEcBmxre1bL4UdIWgv4HfBN4P4RjmmN/VQVymkrrjFx30RExAiquvAqaQ/ga8BU4Lu2vzTs9c2AEylGN6YCh9k+a6yYk3nhdfdym1c+n06R9G8bfqAkAT8Cvmr7SkkzxgrcWoVyzVU2bHAViYjoN7YYcO89eUlTgW8BuwF3AHMknWF7QcthhwOn2P4vSdsAZwEzxoo7mUlewBdtH/e0nSMn8COBO2x/f+KbFRHRm8FqevI7AjfavglA0knA3kBrkjcwNFyxJnBXu6CTmeTPBT4r6ce2H5W0MbAYeARYfeggSW8EXgvs2nLu046JiGiK4sJrx6l0XUlzW54fX45EAGwM3N7y2h3ATsPOPxI4T9KHgdUocuWYJivJ2/Z5krYGLi1GY3gUOMj2XyVdIuk64GxgNsU3e0V53Bm2j2g9JhdeI6Ipurzwep/t2T283duAE2x/RdJLgR9K2tYevYDThCd5Sc+ivC+r7a9RXFR4Gttvbxenk2MiIuowUM0c+DuBTVueb1Lua/VuYA8A25dKmgasC/x9tKATOu9H0kbApcCxE/k+ERF1GVrx2snWxhxgpqTnSFoJOAA4Y9gxtwGvAShHRqYB944VdEJ78rbvAracyPd4hiUD6L6FPYdZ6eF1KmgM3LTvtJ5j6F+fX0FLYMuP3t7+oA4MLHygkjge7NOJUDM27jnE1DvuqaAhQEX3Hh146KFK4vSrwQpm19heIulDFNcvpwLfs329pKOAubbPAP4d+I6kj1GMFB1sj/2PnNo1ERE9KAqUVTMoUs55P2vYviNaHi8AXt5NzCT5iIgeGLG4ppIFnUiSj4jogU0li6EmSpJ8RERPVNViqAkxrj8/kmaUc9YnjaRZkt4wme8ZEdGOKXrynWx1aO5njGeaBSTJR0TjVDSFckL0/K6SnitpnqSdJF1aPv6DpOeXrx8s6TRJ50j6i6SjW859VNLnJV0t6TJJG5T795N0Xbn/4nLO6FHA/pLmS9q/13ZHRFTBdHbDkLpuGtLTmHyZyE8CDgZuBnYp53q+FvgCsE956CzgRcAi4AZJ37B9O0Xthctsf6pM/u8FPgccAbzO9p2S1rL9pKQjgNm2PzRCO5aWGp4yvZdvKSKiKwYWd167ZtL10rL1gF8Ab7G9QNKmwImSZlJ83yu2HHu+7YcAJC0ANqcoxPMkcGZ5zJUUJTYBLgFOkHQKcFq7hjyt1PCK6/fpCpuIaCY1+kbevQzXPESxxHbn8vlngQttbwu8kWK57ZBFLY8HWPrHZXHLaq2n9tt+P0Xd5E2BK8v6NxERjWOKFa+dbHXopSf/JPBm4FxJj1LUNh4qpnNwL42StIXty4HLJb2eItmn3HBENFK/9uSx/RiwF/AxYD7wRUnz6H3+/TGSri2naf4BuBq4ENgmF14jokls9V9P3vYtwLbl4weBHcqXPtNy2OHl6ycAJ7Scu1fL4+ktj08FTi0fv2WEt13Y8j4REY1QXHhNWYNJ4yVLKqmSuMYFN1TQGlhz3lq9B1n0ZO8xgCUVVOes1Oj3OVi23dr2jmxtDf7jiQoaQv/+jBulmnu8TpS+S/IREZOpuPDa3DH5JPmIiB7VtZq1E0nyERE9GFrx2lRJ8hERPeriRt6TrpFJXtKRwKO2c2/YiGg0GxYPJslHRPSlYrimuUm+MS2T9ClJf5b0e2CoguVFkmaXj9eVdEudbYyIGMlAWb+m3VaHRvTkJb0EOICiWuUKwFUUBcs6PX9pFUpWnYgmRkSMKFMoO7MLcLrtxwEkndHNya1VKNfQOqlCGRGTqNnDNU1J8qNZwtIhpWljHRgRUZe+u8frBLgY+GdJq0hanaJUMcAtwEvKx/vW0bCIiLEUs2umdrTVoRFJ3vZVwMkU1SbPBuaULx0LfKCsbLluTc2LiBhVX9/+r0q2Pw98foSXtmt5fPgkNSciomNNHq5pTJKPiFgWZXZNDTzY+wQbP7Go/UEdGFh/jZ5jrHD3gxW0BKasUtG165kzKgmjv95WSZyBxx6vJI6mVPQfdUoFo6AVlQjWyitXEsePV/Mz7leZXRMR0adssSRJPiKif2W4JiKiT2VMPiKizzU5yTd3IGkYSetJulzSPEm71N2eiAjIPPkqvQa41vZ76m5IRESrJs+Tr7UnL+nnkq6UdH1ZSRJJj7a8vq+kEyTNAo4G9pY0X9IqdbU5IqKVDUsGp3S01aHunvy7bC8sk/YcST8b6SDb8yUdAcy2/aHhr6fUcETUqclj8nUn+Y9IenP5eFNg5niCpNRwRNQlN/IehaRXAa8FXmr7cUkXUZQTbk3SKS8cEY3nBif5Osfk1wQeKBP8VsA/lfvvkbS1pCnAm0c/PSKiGQZRR1sd6kzy5wArSPoj8CXgsnL/YcCZwB+Au2tqW0RER2wqm0IpaQ9JN0i6UdJhoxzzVkkLygkrP2kXs7bhGtuLgNeP8vKpIxx/AnDCBDYpImIcxEAFM2ckTQW+BewG3EExGeUM2wtajpkJfBJ4ue0HJK3fLm7dF14nRgUV/AYrqrqnK67rOYZXqWbG6JSNnl1JnMc3mV5JnPt23679QR3Y5JvzKonjxUsqiTPw8CM9x9DUiu4iNFBNNcsYW0Vj8jsCN9q+CUDSScDewIKWY94LfMv2A8X7+u/tgi4zK14jIppoqHZNBcM1GwO3tzy/o9zXaktgS0mXSLpM0h7tgvZnTz4iYrK4GJfv0LqS5rY8P76cAt6pFSimmr8K2AS4WNILbY9604kk+YiIHnUxc+Y+27NHee1OivVCQzYp97W6A7jc9mLgZkl/pkj6cxhFhmsiInrg8sJrJ1sbc4CZkp4jaSXgAOCMYcf8nKIXj6R1KYZvbhoraG1JXtJZktaq6/0jIqpid7aNHcNLgA8B5wJ/BE6xfb2koyS9qTzsXOB+SQuAC4FDbd8/Vtw6p1C+oa73joioUlUrXm2fBZw1bN8RLY8NfLzcOjIpPXlJB0m6oqwgeZykqZJukbSupBmS/ijpO+Xk/vOGqkxK2kLSOWWlyt+VK2MjIhqj6KWro60OE57kJW0N7E8xeX8WMAAcOOywmRRzP18APAjsU+4/Hviw7ZcAnwC+Pcp7HCJprqS5i1k0Ed9GRMSolvebhrwGeAnF6i2AVYDhE/hvtj2/fHwlMEPSdOBlwE/L8wBWHukNUoUyIurUxRTKSTcZSV7AibY/+bSd0sEtT1u73wMUfwimAA+Wvf+IiEYyYrCmG4J0YjJadj6w71CNBUnrSNq83Um2H6aYB7pfeZ4kbT+xTY2I6J473Oow4Um+LK5zOHCepGuAXwMbdnj6gcC7JV0NXE9RxyEiojkafuF1UqZQ2j4ZOHnY7hnl1/uAbVuOPbbl8c1A29oMERG1Ws7H5CMi+lqT7wyVJD/R1PuIWFUlcPXQw5XEeWSTikoWb1pNGdyz/3pZ+4M68Lp9/6WSOFOvv7nnGFWUKwbQlOYmn55U8P8KqKQHbmBwsLk/5yT5iIheGEhPPiKify3v8+QjIvpbknxERL+qb3pkJ7q+etGuRLCkEyTtO8L+GZLe3vJ8tqSvd/v+ERGN0+DVUF315FUUkdnLHtedsmcAbwd+AmB7LjB3rBMiIhrP4AbPrmnbky974DdI+gFwHTBQ3pEESe+QdI2kqyX9sOW0V0j6g6SbWnr1XwJ2KcsNf0zSqySdWcZZT9Kvy1LD35V0a8t7PKNMcaU/gYiInqnDbfJ1OlwzE/h2WQr4VgBJL6AoV/Bq29sDH205fkNgZ2AviuQOcBjwO9uzbP/vYfE/DVxQxj8V2Kx8j07KFKfUcETUqw+Ga261PXzFyauBn9q+D8D2wpbXfl4O6SyQtEEH8XcG3lzGOUfSA+X+TsoUp9RwRNSrwVmn0yT/WJdxW7vTvXxGGbFMcUREYzR8MVQva4MvAPaT9CwoSgi3Of4RYPVRXrsEeGsZZ3dg7XL/uMoUR0RMpipu5D1Rxp3kbV8PfB74bVkK+KttTrmG4qLt1ZI+Nuy1zwC7S7oO2A/4G/BIj2WKIyImx6A622rQdrjG9i08vRTwjJbHJwInDjv+4GHPp5dfF1OM47e6qPz6EPA620skvRTYwfai8ryRyhRHRDSG+mBMfqJtBpwiaQrwJPDenqJVUaFuXEsBnmnqs9Zuf1A7a4w2ytWde3dZv5I4gxVNYt3y+9VUxdzzc6+tJM5tH1y1kjgzHtu49yDX/Kn3GMBgRRVMG6ei/5+VqPO2Tx1oRJK3/RfgRXW3IyKie2r0hddGJPmIiGVaevIREX2sQaNHwyXJR0T0oo/nybcl6WBJ3+zynCMlfWKi2hQRUTW5s60O6clHRPSqwWPy4+rJS1pN0q/KhU3XSdpf0g5l5cmry6qRQ/P+NpJ0jqS/SDq6JcajLY/3lXTCCO+zRXnulZJ+J2mr8bQ3ImJ5Nd6e/B7AXbb3BJC0JjAP2N/2HElrAP8oj51FMT1yEXCDpG/Yvr3D9zkeeL/tv0jaCfg2z1xQhaRDgEMAplHNXOeIiE7142Koa4GvSPoycCbwIHC37TkAth8GKCtHnm/7ofL5AmBzoG2SlzQdeBnw0zIOwMojHZsqlBFRG1NbyYJOjCvJ2/6zpBcDbwA+R1GsbDStFSkHWt6zNRlPG+G8KcCDZR35iIjmanDXcrxj8hsBj9v+EXAMsBOwoaQdytdXl9TuD8g9krYuSxm8efiL5aeBmyXtV8aUpO3H096IiInUj7NrXggcI2kQWAx8gKL2+zckrUIxHt+uoMhhFEM991Lc63X6CMccCPyXpMOBFYGTgKvH2eaIiInR4J78eIdrzgXOHeGlfxr2/IRyGzpvr5bHp1Lc6m947CNbHt9McZE3IqK5+i3JR0REoc6hmE4kyY+minLFgBc92XuM6SNOKura/bOq+U3c8j/nVxJn8Ilqbro+WFHZ2Rlf7vYulyObsm67m6R1YJWR5iKMw0A1P5vBRU9UEqdv9dvsmoiIWCo9+YiIftbgJD+hBcoiIvpeh9MnO+ntS9pD0g2SbpR02BjH7SPJkma3i5kkHxHRK3e4jUHSVOBbwOuBbYC3SdpmhONWBz4KXN5J0yYtyUs6QdK+k/V+ERGTRYOdbW3sCNxo+ybbT1KsC9p7hOM+C3wZ6OhqeHryERGTZ11Jc1u2Q1pe25in1/W6o9z3lLKczKa2f9XpG/Z04VXS/wIOoli1ejtwJXA6xUeO9YDHgffaHrr1/CskfRx4NvAf5YIoJB0KvJWiANnptj8taQZwNvB7ikJldwJ72x6qbhkR0QydX3i9z3bbcfSRlCVgvgoc3M154+7Jl3Vq9gG2pxhDGmr48cCHbb8E+ARFeeAhGwI7A3sBXyrj7A7MpPioMgt4iaRXlMfPBL5l+wUUlS73GaUthwz9ZVxMNXOvIyI6Ut2F1zuBTVueb1LuG7I6sC1wkaRbKCoMnNHu4msvPfmXA7+w/QTwhKRfUlSTHKs88M9tDwILJG1Q7tu93OaVz6dTJPfbgJttD628uRKYMVJDUmo4ImpVTdaZA8yU9ByK5H4A8Pan3qIo2b7u0HNJFwGfsD13rKBVz5NvVx64tZutlq9ftH1c64HlcM3wMsWrVNPMiIgKVZDkbS+R9CGKumBTge/Zvl7SUcBc22eMJ24vF14vAd4oaVp5g4+9KMbguy0PfC7wrjIGkjaWtH4P7YqImDSistk12D7L9pa2t7D9+XLfESMleNuvateLhx568uVt/s4ArgHuobhb1EN0WR7Y9nmStgYuLYd4HqW4mDsw3rZFREyaPi9QdqztIyWtClwMXDlaeWDbBw97Pr3l8deAr40Qf9uWY47tsa0REROjj5P88eWKrGnAibavqqBNvauoKmEVBh58sPcg8yqIATxvfjXLIprz0y1o6tRK4tzzzmruNLnSw73/j1/7+jUraAnwp5uriVOViqq7Nun/ONC/Sd7229sfFRHR3/p5uCYiIpLkIyL6lDubOVOXJPmIiF6lJx8R0b8yJh8R0c+S5CMi+lQHNwSpU18k+bIm8yEA01i15tZExPJEZLhmwqUKZUTUKUk+IqKfNTjJL3O3/5N0vqSN2x8ZETFJKriR90RZpnry5e2vngcsrLstERFA31ehnGzbAD/LfV4jolGS5Kth+zrg43W3IyKiVcoaRIykonKxHqzm0tKGp1dTlvfR2Zu2P6iNG963WgUtgQ0ubndjts6sddr89gd1YMraa1USx488WkkcHq4mTIZrIiL6VRZDRUT0uST5iIj+lBWvERF9ToPNzfLLzGIoSbdImiHporrbEhHxlE4XQmUxVETEsinDNdW4Fxggq10jommS5Htne4fy4VuGv5ZSwxFRpyb35JeZMfmx2D7e9mzbs1dk5bqbExHLm4zJR0T0KaesQURE38o8+YiIfufmZvkk+YiIHqUnH81QUdXHxqno+1py192VxFntwt4rJG55z3MqaAk89Lxqqlne8sPnVRLHf1y9kjjPPfb6SuJUIgXKIiL6Wy68RkT0sST5iIh+ZXLhNSKinzX5wmujVrxKmiXpDXW3IyKiKw1e8dqoJA/MApLkI2KZMbQYqpOtDpUleUmrSfqVpKslXSdpf0lHSJpTPj9ekspjd5B0jaT5ko4pX18JOArYv9y/fxnze5KukDRP0t5VtTciohI2Guxsq0OVPfk9gLtsb297W+Ac4Ju2dyifrwLsVR77feB9tmdRlA/G9pPAEcDJtmfZPhn4FHCB7R2BXYFjJD1j4q+kQyTNlTR3MYsq/JYiIjpQ0XCNpD0k3SDpRkmHjfD6x5FwLBAAAAYJSURBVCUtKDvJ50vavF3MKpP8tcBukr4saRfbDwG7Srpc0rXAq4EXSFoLWN32peV5Pxkj5u7AYZLmAxcB04DNhh+UKpQRUacqhmskTQW+Bbwe2AZ4m6Rthh02D5htezvgVODodm2rbHaN7T9LejHFmPrnJJ0PfLBs0O2SjqRI0t0QsI/tG6pqZ0REpQxUMxSzI3Cj7ZsAJJ0E7A0seOqt7Atbjr8MOKhd0CrH5DcCHrf9I+AY4MXlS/dJmg7sWzbyQeARSTuVrx/QEuYRoHXd87nAh1vG8l9UVXsjIirT+XDNukNDy+V2SEuUjYHbW57fUe4bzbuBs9s1rcp58i+kGDMfBBYDHwD+GbgO+BswZ1jjvlMe+1vgoXL/hSwdnvki8Fng/wDXSJoC3MzScf2IiEboYubMfbZn9/x+0kHAbOCV7Y6tcrjmXIqed6u5wOEjHH59OaZEeXFhbhljIbDDsGPfV1UbIyImQkUzZ+4ENm15vkm57+nvJb2WYlLKK223nWlS14rXPSV9snz/W4GDa2pHRERvqlvoNAeYKek5FMn9AODtrQeUQ9bHAXvY/nsnQWtJ8uX0yJPreO+IieZFvU/jnbrwsQpaAis8Uc2N7df45fRK4qz94zntD+rAwMBAJXGqUCyG6j3L214i6UMUIyJTge/Zvl7SUcBc22dQXO+cDvy0vFR5m+03jRU3tWsiInpVURVK22cBZw3bd0TL49d2GzNJPiKiR1X05CdKknxERC9yZ6iIiH5WX12aTiTJR0T0KsM11ZI01XZzLq9HxPLLzb79X9PqyQPFaq6yvPB8ScdJmirpUUlfkXQ18NK62xgR8RS7s60GjUvykrYG9gde3lKK+EBgNeDyspTx74edk1LDEVGfBt8ZqonDNa8BXgLMKSf7rwL8nSLZ/2ykE2wfDxwPsIbWae7gWET0JQ02d7ymiUlewIm2P/m0ndInMg4fEY1jKlsMNREaN1wDnA/sK2l9AEnrdHL3k4iIOggjd7bVoXE9edsLJB0OnFeWF15McfORiIhmyhTK7oxSwKyaCkkREVVLko9Yfgw++WTvQW69vf0xHVj9/oWVxNGKK1YS5/HXNezmbmee0nuMho/JJ8lHRPQos2siIvpWfQudOpEkHxHRC5MkHxHR15o7WpMkHxHRq9w0JCKinyXJR0T0KRsGmjte0xdJXtIhwCEA06jm7vQRER1LT35ipQplRNQqST4iok8ZyD1eIyL6lcHNHZNvYqnhUUk6S9JGdbcjIuIpprjw2slWg2WqJ2/7DXW3ISLiGTImHxHRx5LkI6IbHqjmTpeq6oLgtJUrCfPIxtWknFe+//JK4lx8ZhVRUqAsIqJ/GUip4YiIPpaefEREv0pZg4iI/mVw5sk/k6QDJH2qrvePiKjMoDvbajBpSV7SSpJWa9n1euCcDo+NiGguu7OtBhOe5CVtLekrwA3AluU+AbOAqyS9UtL8cpsnaXVgbeB6ScdJ2mGi2xgRMW52Mbumk60GE5LkJa0m6Z2Sfg98B1gAbGd7XnnIi4CrbRv4BPBB27OAXYB/2L4HeD5wIfD5Mvl/RNI6o7zfIZLmSpq7mEUT8S1FRIyuwT35ibrwejdwDfAe238a4fU9gLPLx5cAX5X0Y+A023cA2F4EnAScJGkz4JvA0ZKea/uu1mApNRwR9XFli9cmwkQN1+wL3AmcJukISZsPe3134DwA218C3gOsAlwiaauhgyStL+nfgV8CU4G3A/dMUJsjIro3VGq4oRdeJ6Qnb/s84DxJzwIOAn4h6T6KZP4AsILt+wEkbWH7WuDacvx9K0l3AycCWwE/BN5g+86JaGtERM8aPIVyQufJl4n8a8DXJO0IDAC7Ab9pOezfJO0KDALXUwzjTAO+DlxYjttHRDSSAeemIWD7CgBJnwa+27L/wyMcvgi4YJKaFhExfm72TUMmfcWr7fdM9ntGREykJl94Vb+Nhki6F7i17nZExDJhc9vr9RJA0jnAuh0efp/tPXp5v271XZKPiIillql7vEZERHeS5CMi+liSfEREH0uSj4joY0nyERF97P8D/+KtGtfT4i8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Vj7CC1U8_Z"
      },
      "source": [
        "# Congratulations! You've finished this notebook.\n",
        "\n",
        "What didn't we cover?\n",
        "\n",
        "- Subwords / Byte Pair Encoding [[paper]](https://arxiv.org/abs/1508.07909) [[github]](https://github.com/rsennrich/subword-nmt) let you deal with unknown words. \n",
        "- You can implement a [multiplicative/bilinear attention mechanism](https://arxiv.org/abs/1508.04025) instead of the additive one used here.\n",
        "- We used greedy decoding here to get translations, but you can get better results with beam search.\n",
        "- The original model only uses a single dropout layer (in the decoder), but you can experiment with adding more dropout layers, for example on the word embeddings and the source word representations.\n",
        "- You can experiment with multiple encoder/decoder layers.- Experiment with a benchmarked and improved codebase: [Joey NMT](https://github.com/joeynmt/joeynmt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBj5E5G5U8_Z"
      },
      "source": [
        "If this was useful to your research, please consider citing:\n",
        "\n",
        "> J Bastings. 2018. The Annotated Encoder-Decoder with Attention. https://bastings.github.io/annotated_encoder_decoder/\n",
        "\n",
        "Or use the following `Bibtex`:\n",
        "```\n",
        "@misc{bastings2018annotated,\n",
        "  title={The Annotated Encoder-Decoder with Attention},\n",
        "  author={Bastings, J.},\n",
        "  journal={https://bastings.github.io/annotated\\_encoder\\_decoder/},\n",
        "  year={2018}\n",
        "}```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsOhKAoJ559n"
      },
      "source": [
        "# Saving final weight files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmCbAAG-55g_"
      },
      "source": [
        "torch.save(model.state_dict(), 'de_eng_translation_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58_jzn7D7A34"
      },
      "source": [
        "# Saving Traced model for Lambda Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yr9Oy687CWH"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "max_len=100\n",
        "\n",
        "model.load_state_dict(torch.load('de_eng_translation_model.pt', map_location=device))\n",
        "test_seq = torch.LongTensor(max_len, 1).random_(0, INPUT_DIM).to(device)\n",
        "traced_model = torch.jit.trace(model,test_seq)\n",
        "traced_model.save('tut3-model_cpu.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mld08LltoA0P"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('TRG_vocab.pickle', 'wb') as handle:\n",
        "    pickle.dump(TRG.vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('SRC_vocab.pickle', 'wb') as handle:\n",
        "    pickle.dump(SRC.vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQxHCs5K8Ohh",
        "outputId": "53fbbdd9-871e-4dfa-f1aa-a612ef443535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SRC.vocab.__getattribute__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<method-wrapper '__getattribute__' of Vocab object at 0x7faa9831e5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uejoKECk9OUX"
      },
      "source": [
        "with open('SRC_vocab_stoi.pickle', 'wb') as handle:\n",
        "    pickle.dump(SRC.vocab.stoi, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EBGMnouDa9N"
      },
      "source": [
        "## Reading Text from user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cume14tExSW0",
        "outputId": "cf7a48e3-cb08-486e-c4a7-49f24d34de58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "src = input(\"Enter the german text: \")\n",
        "tokenized = [tok.text for tok in spacy_de.tokenizer(src)]\n",
        "tokenized.append(\"</s>\")\n",
        "# print(tokenized)\n",
        "indexed = [[SRC.vocab.stoi[t] for t in tokenized]]\n",
        "srcpr = [lookup_words(x, SRC.vocab) for x in indexed]\n",
        "# print(\"German : \",[\" \".join(y) for y in srcpr][0])\n",
        "srcs = torch.LongTensor(indexed).cuda()\n",
        "length = torch.LongTensor([len(indexed[0])]).cuda()\n",
        "mask = (srcs != 0).unsqueeze(-2).cuda()\n",
        "# print(srcs)\n",
        "# print(mask)\n",
        "# print(length)\n",
        "pred, attention = greedy_decode(\n",
        "  model, srcs, mask, length, max_len=25,\n",
        "  sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "  eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "# print(pred)\n",
        "english = lookup_words(pred, TRG.vocab)\n",
        "print(\"English: \", \" \".join(english))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the german text: genau wie ich haben sie große träume . \n",
            "English:  and as i have , they have big dreams .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFBqTMSxDSWe"
      },
      "source": [
        "## Random input from samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXRSUwv6sceE",
        "outputId": "d6768060-3482-46e3-e9fc-2ab54023526a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "rand = random.randint(1, len(valid_iter))\n",
        " \n",
        "for batch in valid_iter:\n",
        "  rand -=1\n",
        "  if(rand == 0):\n",
        "    break\n",
        "\n",
        "batch = rebatch(PAD_INDEX, batch)\n",
        "srcpr = [lookup_words(x, SRC.vocab) for x in batch.src]\n",
        "print(\"German : \",[\" \".join(y) for y in srcpr][0][:-4])\n",
        "# print(batch.src)\n",
        "# print(batch.src_mask)\n",
        "pred, attention = greedy_decode(\n",
        "  model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
        "  sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "  eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "english = lookup_words(pred, TRG.vocab)\n",
        "print(\"English: \", \" \".join(english))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German :  ihr name war <unk> und sie war wie eine mutter für mich . ihr tod kam plötzlich und unerwartet . \n",
            "English:  her name was <unk> , and she was a mom for me , her afghan and unexpectedly .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}