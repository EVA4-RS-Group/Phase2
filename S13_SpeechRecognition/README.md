# Session 13 - AI for Sound


## 1. Executive Summary
**Group Members:** *Ramjee Ganti, Srinivasan G, Roshan, Dr. Rajesh and Sujit Ojha*

### **Objectives**:

- Train both the models. Try to move the one you like to Lambda, and as usual, your submit your Lambda link. 

### **Results**:

- Team hosted static website : http://rsgroup.s3-website.ap-south-1.amazonaws.com/
- Website results
    - <img src="results/website_snapshot_1.png" alt="Set1" height="350"/><img src="results/website_snapshot_2.png" alt="set2" height="350"/>
- Colab results
    - <img src="results/colab_snapshot.png" alt="Set1" height="350"/>

### **Key Highlights**:
- Training Model1: "Basic Audio Processing and a Simple Model" [EVA4P2_Session13_Speech_recognition_model1_training_v1.ipynb](EVA4P2_Session13_Speech_recognition_model1_training_v1.ipynb)
    - Best accuracy Training: 94.51%, Validation: 91.04%, Test: 91.075%
- Training Model2: "Deep Speech 2" [Building_an_end_to_end_Speech_Recognition_model_in_PyTorch.ipynb](Building_an_end_to_end_Speech_Recognition_model_in_PyTorch.ipynb)

## 2. AI for Sound



## 3. Steps (Developer Section)

**MODEL1**
- Training [Notebook](EVA4P2_Session13_Speech_recognition_model1_training_v1.ipynb)
- Deployment [Notebook](EVA4P2_Session13_Speech_recognition_model1_inference_v1a.ipynb), [handler.py](NeuralEmbedding-Deployment/handler.py) and [serverless.yml](NeuralEmbedding-Deployment/serverless.yml)
    - Using serverless, python-plugin-requirements

**MODEL2**

## 4. References

1. [Basic Audio Processing and a Simple Model](https://colab.research.google.com/drive/1z6Ia_zT9HbAd6zxpafDVzd1Q0klMGaA4?usp=sharing)
2. [Building an end-to-end Speech Recognition model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch)
3. [Deep Speech 2, Colab](https://colab.research.google.com/drive/1Z-4MiFimY9JPWk93V0MwblXu2iS8Lzp0?usp=sharing)
4. [EVA4 Phase2 Session 13 - AI for Sound](https://theschoolof.ai/)

