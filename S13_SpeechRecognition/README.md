# Session 13 - AI for Sound


## 1. Executive Summary
**Group Members:** *Ramjee Ganti, Srinivasan G, Roshan, Dr. Rajesh and Sujit Ojha*

### **Objectives**:

- Train both the models. Try to move the one you like to Lambda, and as usual, your submit your Lambda link. 

### **Results**:

- Team hosted static website : http://rsgroup.s3-website.ap-south-1.amazonaws.com/
- Website results
    - <img src="results/website_snapshot_1.png" alt="Set1" height="300"/><img src="results/website_snapshot_2.png" alt="set2" height="300"/>
- Colab results
    - <img src="results/colab_snapshot.png" alt="Set1" height="400"/>

## 2. AI for Sound



## 3. Steps (Developer Section)

- Training & Saving Model [Notebook](EVA4P2_S11_annotated_encoder_decoder_deployment_v2.ipynb)
    - Based on Ref #1: [The Annotated Encoder Decoder](https://bastings.github.io/annotated_encoder_decoder/)
- Deployment [Notebook](EVA4P2_S11_annotated_encoder_decoder.ipynb), [handler.py](NeuralEmbedding-Deployment/handler.py) and [serverless.yml](NeuralEmbedding-Deployment/serverless.yml)
    - Using serverless, python-plugin-requirements

## 4. References

1. [Basic Audio Processing and a Simple Model](https://colab.research.google.com/drive/1z6Ia_zT9HbAd6zxpafDVzd1Q0klMGaA4?usp=sharing)
2. [Building an end-to-end Speech Recognition model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch)
3. [Deep Speech 2, Colab](https://colab.research.google.com/drive/1Z-4MiFimY9JPWk93V0MwblXu2iS8Lzp0?usp=sharing)
4. [EVA4 Phase2 Session 13 - AI for Sound](https://theschoolof.ai/)

