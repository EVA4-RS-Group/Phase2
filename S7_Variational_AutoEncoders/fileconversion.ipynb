{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxO4kxa7SK9bEC1UiCv/YL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EVA4-RS-Group/Phase2/blob/master/S7_Variational_AutoEncoders/fileconversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUCQRVpqrEkl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxu3xLlOEr5Z",
        "outputId": "85b751d7-e71b-4c88-c632-3e15edbac540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHFaRmqEuCf"
      },
      "source": [
        "import torch\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unIHctqxwVuf",
        "outputId": "d32d449f-5803-47a9-9789-e3622c26ceca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApK2Ku5ZzVKJ"
      },
      "source": [
        "!mkdir samples"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ny80Yo6SbY"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions import kl_divergence\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8diuAs_7IDO"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        try:\n",
        "            nn.init.xavier_uniform_(m.weight.data)\n",
        "            m.bias.data.fill_(0)\n",
        "        except AttributeError:\n",
        "            print(\"Skipping initialization of \", classname)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRz3IzS96Kl0"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, dim, z_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, dim, 4, 2, 1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(dim, dim, 4, 2, 1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(dim, dim, 5, 1, 0),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(dim, z_dim * 2, 3, 1, 0),\n",
        "            nn.BatchNorm2d(z_dim * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, dim, 3, 1, 0),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(dim, dim, 5, 1, 0),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(dim, dim, 4, 2, 1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(dim, input_dim, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x).chunk(2, dim=1)\n",
        "\n",
        "        q_z_x = Normal(mu, logvar.mul(.5).exp())\n",
        "        p_z = Normal(torch.zeros_like(mu), torch.ones_like(logvar))\n",
        "        kl_div = kl_divergence(q_z_x, p_z).sum(1).mean()\n",
        "\n",
        "        x_tilde = self.decoder(q_z_x.rsample())\n",
        "        return x_tilde, kl_div"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWFXnMkA69NC"
      },
      "source": [
        "DEVICE1 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14NYv0ac8XzW",
        "outputId": "82583348-f265-41a9-b6bb-4d249a596e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(DEVICE1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqelGvHn6xEs"
      },
      "source": [
        "INPUT_DIM = 3\n",
        "DIM = 6\n",
        "Z_DIM =  8\n",
        "model = VAE(INPUT_DIM, DIM, Z_DIM)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYXl3BVM6mWh"
      },
      "source": [
        "DEVICE2 = torch.device(\"cpu\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XObcVotG7OxN",
        "outputId": "d5def9d6-ea03-4db7-e130-3b9ace97f8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('VAE.pt',map_location=DEVICE2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYeT66wIrIEV",
        "outputId": "db7d99a3-0e01-411c-be2b-d73a873f3c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "model.eval()\n",
        "# trace model with a dummy input\n",
        "traced_model = torch.jit.trace(model, torch.randn(1, 3, 180, 180))\n",
        "traced_model.save('VAE_jit2.pt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1037: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n",
            "\t%eps : Float(1, 8, 39, 39) = aten::normal(%283, %293, %294) # /usr/local/lib/python3.6/dist-packages/torch/distributions/utils.py:40:0\n",
            "This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 0, 13, 159] (-0.09480638056993484 vs. -0.25702783465385437) and 93781 other locations (96.48%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OX31y26CCpJ"
      },
      "source": [
        "# import torch.onnx\n",
        "\n",
        "# # Input to the model\n",
        "# x = torch.randn(1, 3, 180, 180, requires_grad=True)\n",
        "# torch_out = model(x)\n",
        "\n",
        "# # Export the model\n",
        "# torch.onnx.export(model,               # model being run\n",
        "#                   x,                         # model input (or a tuple for multiple inputs)\n",
        "#                   \"VAE.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
        "#                   opset_version=12,          # the ONNX version to export the model to\n",
        "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "#                   input_names = ['input'],   # the model's input names\n",
        "#                   output_names = ['output'], # the model's output names\n",
        "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
        "#                                 'output' : {0 : 'batch_size'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFDCukuD4HSX",
        "outputId": "9b045271-7e72-4327-8eec-8f460787620f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "model.save(\"VAE_jit3.pt\" )\n",
        "torch.jit.save()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-54f12f6bfac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VAE_jit3.pt\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 594\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az5GFmVMBCuH"
      },
      "source": [
        "model = torch.jit.load(\"VAE_jit2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjAMdYcyXE8"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWRrP_Tsyc8S"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
        "                                std=(0.5, 0.5, 0.5)) ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOEeD4PkwvzX"
      },
      "source": [
        "dataset_test = datasets.ImageFolder('data/test/', transform=transform)\n",
        "test_loader = DataLoader(dataset=dataset_test, batch_size=16, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BFfn9SbxHBH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR3qGO5AsbBz"
      },
      "source": [
        "\n",
        "def generate_reconstructions():\n",
        "    model.eval()\n",
        "    x, _ = test_loader.__iter__().next() \n",
        "\n",
        "    # x = x.to(DEVICE)\n",
        "    # x = x[:16].to(DEVICE)\n",
        "    out, kl_div = model(x)\n",
        "    x = (x.data + 1) / 2\n",
        "    out =  (out.data + 1) / 2\n",
        "    save_image(\n",
        "        out,\n",
        "        'samples/Reconstruction.png',\n",
        "        nrow=8\n",
        "    )\n",
        "    save_image(\n",
        "        x,\n",
        "        'samples/sample_input.png',\n",
        "        nrow=8\n",
        "    )\n",
        "generate_reconstructions()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}